<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 2 Regularized logistic regression | Machine Learning Notes - Andrew Ng</title>
  <meta name="description" content="Study material for ML - Stanford University" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="9 2 Regularized logistic regression | Machine Learning Notes - Andrew Ng" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Study material for ML - Stanford University" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 2 Regularized logistic regression | Machine Learning Notes - Andrew Ng" />
  
  <meta name="twitter:description" content="Study material for ML - Stanford University" />
  

<meta name="author" content="Fikir Worku Edossa" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ml-ex-2-r-implementationa-logistic-regression.html"/>
<link rel="next" href="ml-programming-exercise-3.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Week 1</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#supervised-learning-note"><i class="fa fa-check"></i><b>1.2.1</b> Supervised Learning (note)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.3</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#model-representation"><i class="fa fa-check"></i><b>1.4</b> Model Representation</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#cost-function"><i class="fa fa-check"></i><b>1.5</b> Cost Function</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#cost-function---intution-i"><i class="fa fa-check"></i><b>1.6</b> Cost Function - Intution I</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#cost-function---intution-ii"><i class="fa fa-check"></i><b>1.7</b> Cost Function - Intution II</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#gradient-descent"><i class="fa fa-check"></i><b>1.8</b> Gradient Descent</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#gradient-descent-intution"><i class="fa fa-check"></i><b>1.9</b> Gradient Descent Intution</a></li>
<li class="chapter" data-level="1.10" data-path="index.html"><a href="index.html#gradient-descent-for-linear-regression"><i class="fa fa-check"></i><b>1.10</b> Gradient Descent for Linear Regression</a></li>
<li class="chapter" data-level="1.11" data-path="index.html"><a href="index.html#matrices-and-vectors"><i class="fa fa-check"></i><b>1.11</b> Matrices and Vectors</a></li>
<li class="chapter" data-level="1.12" data-path="index.html"><a href="index.html#addition-and-scalar-multiplication"><i class="fa fa-check"></i><b>1.12</b> Addition and Scalar Multiplication</a></li>
<li class="chapter" data-level="1.13" data-path="index.html"><a href="index.html#matrix-vector-multiplication"><i class="fa fa-check"></i><b>1.13</b> Matrix-Vector Multiplication</a></li>
<li class="chapter" data-level="1.14" data-path="index.html"><a href="index.html#matrix-matrix-multiplication"><i class="fa fa-check"></i><b>1.14</b> Matrix-Matrix Multiplication</a></li>
<li class="chapter" data-level="1.15" data-path="index.html"><a href="index.html#matrix-multiplication-properties"><i class="fa fa-check"></i><b>1.15</b> Matrix Multiplication Properties</a></li>
<li class="chapter" data-level="1.16" data-path="index.html"><a href="index.html#inverse-and-transpose"><i class="fa fa-check"></i><b>1.16</b> Inverse and Transpose</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-2.html"><a href="week-2.html"><i class="fa fa-check"></i><b>2</b> Week 2</a><ul>
<li class="chapter" data-level="2.1" data-path="week-2.html"><a href="week-2.html#multiple-features"><i class="fa fa-check"></i><b>2.1</b> Multiple features</a></li>
<li class="chapter" data-level="2.2" data-path="week-2.html"><a href="week-2.html#gradient-descent-for-multiple-variables"><i class="fa fa-check"></i><b>2.2</b> Gradient Descent for Multiple Variables</a></li>
<li class="chapter" data-level="2.3" data-path="week-2.html"><a href="week-2.html#gradient-descent-in-practice-i---feature-scaling"><i class="fa fa-check"></i><b>2.3</b> Gradient Descent in Practice I - Feature Scaling</a></li>
<li class="chapter" data-level="2.4" data-path="week-2.html"><a href="week-2.html#gradient-descent-in-practice-ii---learning-rate"><i class="fa fa-check"></i><b>2.4</b> Gradient Descent in Practice II - Learning Rate</a></li>
<li class="chapter" data-level="2.5" data-path="week-2.html"><a href="week-2.html#features-and-polynomial-regression"><i class="fa fa-check"></i><b>2.5</b> Features and Polynomial Regression</a><ul>
<li class="chapter" data-level="2.5.1" data-path="week-2.html"><a href="week-2.html#polynomial-regression"><i class="fa fa-check"></i><b>2.5.1</b> Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="week-2.html"><a href="week-2.html#normal-equation"><i class="fa fa-check"></i><b>2.6</b> Normal Equation</a></li>
<li class="chapter" data-level="2.7" data-path="week-2.html"><a href="week-2.html#normal-equation-non-invertibility"><i class="fa fa-check"></i><b>2.7</b> Normal Equation Non-invertibility</a></li>
<li class="chapter" data-level="2.8" data-path="week-2.html"><a href="week-2.html#octave---basic-operations"><i class="fa fa-check"></i><b>2.8</b> Octave - Basic Operations</a></li>
<li class="chapter" data-level="2.9" data-path="week-2.html"><a href="week-2.html#moving-data-around"><i class="fa fa-check"></i><b>2.9</b> Moving Data Around</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-3.html"><a href="week-3.html"><i class="fa fa-check"></i><b>3</b> Week 3</a><ul>
<li class="chapter" data-level="3.1" data-path="week-3.html"><a href="week-3.html#classification"><i class="fa fa-check"></i><b>3.1</b> Classification</a></li>
<li class="chapter" data-level="3.2" data-path="week-3.html"><a href="week-3.html#logistic-regression---hpothesis-representation"><i class="fa fa-check"></i><b>3.2</b> Logistic regression - Hpothesis Representation</a></li>
<li class="chapter" data-level="3.3" data-path="week-3.html"><a href="week-3.html#decision-boundary"><i class="fa fa-check"></i><b>3.3</b> Decision Boundary</a></li>
<li class="chapter" data-level="3.4" data-path="week-3.html"><a href="week-3.html#logistic-regression---cost-function"><i class="fa fa-check"></i><b>3.4</b> Logistic Regression - Cost Function</a></li>
<li class="chapter" data-level="3.5" data-path="week-3.html"><a href="week-3.html#simplified-cost-functin-and-gradient-descent---logreg"><i class="fa fa-check"></i><b>3.5</b> Simplified Cost Functin and Gradient Descent - LogReg</a><ul>
<li class="chapter" data-level="3.5.1" data-path="index.html"><a href="index.html#gradient-descent"><i class="fa fa-check"></i><b>3.5.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="3.5.2" data-path="week-3.html"><a href="week-3.html#partial-derivative-of-jθ"><i class="fa fa-check"></i><b>3.5.2</b> Partial derivative of J(θ)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="week-3.html"><a href="week-3.html#advanced-optimization"><i class="fa fa-check"></i><b>3.6</b> Advanced Optimization</a></li>
<li class="chapter" data-level="3.7" data-path="week-3.html"><a href="week-3.html#multiclass-classification-one-vs-all"><i class="fa fa-check"></i><b>3.7</b> Multiclass Classification: One-vs-all</a></li>
<li class="chapter" data-level="3.8" data-path="week-3.html"><a href="week-3.html#the-problem-of-overfitting"><i class="fa fa-check"></i><b>3.8</b> The Problem of Overfitting</a></li>
<li class="chapter" data-level="3.9" data-path="week-3.html"><a href="week-3.html#regularization-cost-function"><i class="fa fa-check"></i><b>3.9</b> Regularization Cost Function</a></li>
<li class="chapter" data-level="3.10" data-path="week-3.html"><a href="week-3.html#regularized-linear-regression"><i class="fa fa-check"></i><b>3.10</b> Regularized Linear Regression</a><ul>
<li class="chapter" data-level="3.10.1" data-path="week-3.html"><a href="week-3.html#gradient-descent-1"><i class="fa fa-check"></i><b>3.10.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="3.10.2" data-path="week-2.html"><a href="week-2.html#normal-equation"><i class="fa fa-check"></i><b>3.10.2</b> Normal Equation</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="week-3.html"><a href="week-3.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>3.11</b> Regularized Logistic Regression</a><ul>
<li class="chapter" data-level="3.11.1" data-path="index.html"><a href="index.html#cost-function"><i class="fa fa-check"></i><b>3.11.1</b> Cost Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-4.html"><a href="week-4.html"><i class="fa fa-check"></i><b>4</b> Week 4</a><ul>
<li class="chapter" data-level="4.1" data-path="week-4.html"><a href="week-4.html#model-representation-i"><i class="fa fa-check"></i><b>4.1</b> Model Representation I</a></li>
<li class="chapter" data-level="4.2" data-path="week-4.html"><a href="week-4.html#model-representation-ii"><i class="fa fa-check"></i><b>4.2</b> Model Representation II</a><ul>
<li class="chapter" data-level="4.2.1" data-path="week-4.html"><a href="week-4.html#examples-and-intuitions-i"><i class="fa fa-check"></i><b>4.2.1</b> Examples and Intuitions I</a></li>
<li class="chapter" data-level="4.2.2" data-path="week-4.html"><a href="week-4.html#examples-and-intuitions-ii"><i class="fa fa-check"></i><b>4.2.2</b> Examples and Intuitions II</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="week-4.html"><a href="week-4.html#multiclass-classification"><i class="fa fa-check"></i><b>4.3</b> Multiclass Classification</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-5.html"><a href="week-5.html"><i class="fa fa-check"></i><b>5</b> Week 5</a><ul>
<li class="chapter" data-level="5.1" data-path="week-5.html"><a href="week-5.html#cost-function-and-backpropagation---neural-networks"><i class="fa fa-check"></i><b>5.1</b> Cost Function and Backpropagation - Neural Networks</a><ul>
<li class="chapter" data-level="5.1.1" data-path="index.html"><a href="index.html#cost-function"><i class="fa fa-check"></i><b>5.1.1</b> Cost Function</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="week-5.html"><a href="week-5.html#backpropagation-algorithm"><i class="fa fa-check"></i><b>5.2</b> Backpropagation Algorithm</a><ul>
<li class="chapter" data-level="5.2.1" data-path="week-5.html"><a href="week-5.html#back-propagation-algorithm"><i class="fa fa-check"></i><b>5.2.1</b> Back propagation Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="week-5.html"><a href="week-5.html#backpropagation-intuition"><i class="fa fa-check"></i><b>5.3</b> Backpropagation Intuition</a></li>
<li class="chapter" data-level="5.4" data-path="week-5.html"><a href="week-5.html#implementation-note-unrolling-parameters"><i class="fa fa-check"></i><b>5.4</b> Implementation Note: Unrolling Parameters</a></li>
<li class="chapter" data-level="5.5" data-path="week-5.html"><a href="week-5.html#gradient-checking"><i class="fa fa-check"></i><b>5.5</b> Gradient Checking</a></li>
<li class="chapter" data-level="5.6" data-path="week-5.html"><a href="week-5.html#random-initialization"><i class="fa fa-check"></i><b>5.6</b> Random Initialization</a></li>
<li class="chapter" data-level="5.7" data-path="week-5.html"><a href="week-5.html#putting-it-together"><i class="fa fa-check"></i><b>5.7</b> Putting it Together</a><ul>
<li class="chapter" data-level="5.7.1" data-path="week-5.html"><a href="week-5.html#training-a-neural-network"><i class="fa fa-check"></i><b>5.7.1</b> Training a Neural Network</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="6" data-path="charlie-chapter.html"><a href="charlie-chapter.html"><i class="fa fa-check"></i><b>6</b> Charlie Chapter</a><ul>
<li class="chapter" data-level="6.1" data-path="charlie-chapter.html"><a href="charlie-chapter.html#including-plots"><i class="fa fa-check"></i><b>6.1</b> Including Plots</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="what-happens-here.html"><a href="what-happens-here.html"><i class="fa fa-check"></i><b>7</b> What happens here?</a></li>
<li class="chapter" data-level="8" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html"><i class="fa fa-check"></i><b>8</b> 1 ML Ex 2 R implementationa: Logistic Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#introduction"><i class="fa fa-check"></i><b>8.1</b> 1.1 Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#data-prep"><i class="fa fa-check"></i><b>8.1.1</b> Data Prep</a></li>
<li class="chapter" data-level="8.1.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#visualizing-the-data"><i class="fa fa-check"></i><b>8.1.2</b> Visualizing the data</a></li>
<li class="chapter" data-level="8.1.3" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#warmup-exercise-sigmoid-function"><i class="fa fa-check"></i><b>8.1.3</b> Warmup exercise: sigmoid function</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cost-function-and-gradient"><i class="fa fa-check"></i><b>8.2</b> 1.2 Cost function and gradient</a></li>
<li class="chapter" data-level="8.3" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#learning-parameters-using-fminunc"><i class="fa fa-check"></i><b>8.3</b> 1.3 Learning parameters using fminunc</a></li>
<li class="chapter" data-level="8.4" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#plot-the-decision-boundary"><i class="fa fa-check"></i><b>8.4</b> 1.4 Plot the decision boundary</a></li>
<li class="chapter" data-level="8.5" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#evaluating-logistic-regression"><i class="fa fa-check"></i><b>8.5</b> 1.5 Evaluating logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="week-3.html"><a href="week-3.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>9</b> 2 Regularized logistic regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html"><i class="fa fa-check"></i><b>9.1</b> 2.1 Visualizing the data</a></li>
<li class="chapter" data-level="9.2" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#feature-mapping"><i class="fa fa-check"></i><b>9.2</b> 2.2 Feature mapping</a></li>
<li class="chapter" data-level="9.3" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#cost-function-and-gradient-1"><i class="fa fa-check"></i><b>9.3</b> 2.3 Cost function and gradient</a></li>
<li class="chapter" data-level="9.4" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#learning-parameters-using-fminunc-1"><i class="fa fa-check"></i><b>9.4</b> 2.4 Learning parameters using fminunc</a></li>
<li class="chapter" data-level="9.5" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#plotting-the-decision-boundary"><i class="fa fa-check"></i><b>9.5</b> 2.5 Plotting the decision boundary</a></li>
<li class="chapter" data-level="9.6" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#varying-lambda-levels"><i class="fa fa-check"></i><b>9.6</b> 2.4 Varying lambda levels</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html"><i class="fa fa-check"></i><b>10</b> ML - Programming Exercise 3</a><ul>
<li class="chapter" data-level="10.1" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html#introduction-multi-class-classification"><i class="fa fa-check"></i><b>10.1</b> Introduction: Multi-class Classification</a><ul>
<li class="chapter" data-level="10.1.1" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html#dataset"><i class="fa fa-check"></i><b>10.1.1</b> 1.1 Dataset</a></li>
<li class="chapter" data-level="10.1.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#visualizing-the-data"><i class="fa fa-check"></i><b>10.1.2</b> 1.2 Visualizing the data</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Notes - Andrew Ng</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularized-logistic-regression" class="section level1">
<h1><span class="header-section-number">9</span> 2 Regularized logistic regression</h1>
<p>In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure
it is functioning correctly.</p>
<p>Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="regularized-logistic-regression.html#cb90-1"></a><span class="kw">library</span>(readr)</span>
<span id="cb90-2"><a href="regularized-logistic-regression.html#cb90-2"></a>ex2data2 &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;ex2data2.txt&quot;</span>, </span>
<span id="cb90-3"><a href="regularized-logistic-regression.html#cb90-3"></a>    <span class="dt">col_names =</span> <span class="kw">c</span>(<span class="st">&quot;microChip1&quot;</span>, <span class="st">&quot;microChip2&quot;</span>, <span class="st">&quot;qualityAssurance&quot;</span>),</span>
<span id="cb90-4"><a href="regularized-logistic-regression.html#cb90-4"></a>    <span class="dt">col_types =</span> <span class="kw">cols</span>(<span class="st">`</span><span class="dt">1</span><span class="st">`</span> =<span class="st"> </span><span class="kw">col_factor</span>(<span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;0&quot;</span>, </span>
<span id="cb90-5"><a href="regularized-logistic-regression.html#cb90-5"></a>        <span class="st">&quot;1&quot;</span>))))</span>
<span id="cb90-6"><a href="regularized-logistic-regression.html#cb90-6"></a>X  &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(ex2data2[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>])</span>
<span id="cb90-7"><a href="regularized-logistic-regression.html#cb90-7"></a>y &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.matrix</span>(ex2data2[,<span class="dv">3</span>]))</span>
<span id="cb90-8"><a href="regularized-logistic-regression.html#cb90-8"></a>X &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dt">ones =</span> <span class="kw">rep</span>(<span class="dv">1</span>,<span class="kw">dim</span>(X)[<span class="dv">1</span>]), X)</span>
<span id="cb90-9"><a href="regularized-logistic-regression.html#cb90-9"></a>m &lt;-<span class="st"> </span><span class="kw">dim</span>(X)[<span class="dv">1</span>]</span>
<span id="cb90-10"><a href="regularized-logistic-regression.html#cb90-10"></a>n &lt;-<span class="st"> </span><span class="kw">dim</span>(X)[<span class="dv">2</span>]</span></code></pre></div>
<div id="visualizing-the-data-1" class="section level2">
<h2><span class="header-section-number">9.1</span> 2.1 Visualizing the data</h2>
<p>Similar to the previous parts of this exercise, plotData is used to generate a figure like Figure 3, where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="regularized-logistic-regression.html#cb91-1"></a><span class="co">#par(mar=c(4.1, 5.1, 1.1, 5.1), xpd=TRUE)</span></span>
<span id="cb91-2"><a href="regularized-logistic-regression.html#cb91-2"></a></span>
<span id="cb91-3"><a href="regularized-logistic-regression.html#cb91-3"></a><span class="co"># Plot both groups</span></span>
<span id="cb91-4"><a href="regularized-logistic-regression.html#cb91-4"></a>pch.list &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb91-5"><a href="regularized-logistic-regression.html#cb91-5"></a>pch.list[ex2data2<span class="op">$</span>qualityAssurance <span class="op">==</span><span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="dv">21</span></span>
<span id="cb91-6"><a href="regularized-logistic-regression.html#cb91-6"></a>pch.list[ex2data2<span class="op">$</span>qualityAssurance <span class="op">==</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">43</span></span>
<span id="cb91-7"><a href="regularized-logistic-regression.html#cb91-7"></a></span>
<span id="cb91-8"><a href="regularized-logistic-regression.html#cb91-8"></a><span class="kw">plot</span>(microChip2 <span class="op">~</span><span class="st"> </span>microChip1, ex2data2, <span class="dt">ylim=</span><span class="kw">range</span>(<span class="kw">c</span>(X[,<span class="dv">3</span>], X[,<span class="dv">3</span>])), <span class="dt">xlim=</span><span class="kw">range</span>(<span class="kw">c</span>(X[,<span class="dv">2</span>], X[,<span class="dv">2</span>])), </span>
<span id="cb91-9"><a href="regularized-logistic-regression.html#cb91-9"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">pch =</span> <span class="kw">c</span>(pch.list),<span class="dt">bg=</span> <span class="st">&quot;yellow&quot;</span>, <span class="dt">axes =</span> <span class="ot">FALSE</span>)</span>
<span id="cb91-10"><a href="regularized-logistic-regression.html#cb91-10"></a></span>
<span id="cb91-11"><a href="regularized-logistic-regression.html#cb91-11"></a><span class="co"># set axis </span></span>
<span id="cb91-12"><a href="regularized-logistic-regression.html#cb91-12"></a></span>
<span id="cb91-13"><a href="regularized-logistic-regression.html#cb91-13"></a><span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">at=</span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dt">by =</span> <span class="fl">0.5</span> ))</span>
<span id="cb91-14"><a href="regularized-logistic-regression.html#cb91-14"></a><span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">2</span>, <span class="dt">at=</span><span class="kw">seq</span>(<span class="op">-</span><span class="fl">0.8</span>, <span class="fl">1.2</span>, <span class="dt">by=</span><span class="fl">0.2</span>))</span>
<span id="cb91-15"><a href="regularized-logistic-regression.html#cb91-15"></a></span>
<span id="cb91-16"><a href="regularized-logistic-regression.html#cb91-16"></a><span class="co"># Add legend to top right, outside plot region</span></span>
<span id="cb91-17"><a href="regularized-logistic-regression.html#cb91-17"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">inset=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;y = 0&quot;</span>,<span class="st">&quot;y = 1&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;yellow&quot;</span>,<span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>, <span class="dv">43</span>))</span></code></pre></div>
<p><img src="bookdw_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>The figure above shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.</p>
</div>
<div id="feature-mapping" class="section level2">
<h2><span class="header-section-number">9.2</span> 2.2 Feature mapping</h2>
<p>One way to fit the data better is to create more features from each data point. In the provided function mapFeature.m, we will map the features into all polynomial terms of x1 and x2 up to the sixth power.</p>
<p><span class="math inline">\(\begin{bmatrix} 1 \newline x_1 \newline x_2 \newline x_1^{2} \newline x_1 x_2 \newline x_2^{2} \newline x_1^{3} \newline . \newline . \newline . \newline x_1 x_2^{5} \newline x_2^{6} \end{bmatrix}\)</span></p>
<p>As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.</p>
<p>While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="regularized-logistic-regression.html#cb92-1"></a><span class="co">#  MAPFEATURE Feature mapping function to polynomial features - dope</span></span>
<span id="cb92-2"><a href="regularized-logistic-regression.html#cb92-2"></a><span class="co"># </span></span>
<span id="cb92-3"><a href="regularized-logistic-regression.html#cb92-3"></a><span class="co">#    MAPFEATURE(X1, X2) maps the two input features</span></span>
<span id="cb92-4"><a href="regularized-logistic-regression.html#cb92-4"></a><span class="co">#    to quadratic features used in the regularization exercise.</span></span>
<span id="cb92-5"><a href="regularized-logistic-regression.html#cb92-5"></a><span class="co"># </span></span>
<span id="cb92-6"><a href="regularized-logistic-regression.html#cb92-6"></a><span class="co">#    Returns a new feature array with more features, comprising of </span></span>
<span id="cb92-7"><a href="regularized-logistic-regression.html#cb92-7"></a><span class="co">#    X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..</span></span>
<span id="cb92-8"><a href="regularized-logistic-regression.html#cb92-8"></a><span class="co"># </span></span>
<span id="cb92-9"><a href="regularized-logistic-regression.html#cb92-9"></a><span class="co">#    Inputs X1, X2 must be the same size</span></span>
<span id="cb92-10"><a href="regularized-logistic-regression.html#cb92-10"></a></span>
<span id="cb92-11"><a href="regularized-logistic-regression.html#cb92-11"></a>mapFeature &lt;-<span class="st"> </span><span class="cf">function</span>(X1, X2){</span>
<span id="cb92-12"><a href="regularized-logistic-regression.html#cb92-12"></a>        </span>
<span id="cb92-13"><a href="regularized-logistic-regression.html#cb92-13"></a>        degree &lt;-<span class="st"> </span><span class="dv">6</span></span>
<span id="cb92-14"><a href="regularized-logistic-regression.html#cb92-14"></a>        </span>
<span id="cb92-15"><a href="regularized-logistic-regression.html#cb92-15"></a>        out &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(X1)))</span>
<span id="cb92-16"><a href="regularized-logistic-regression.html#cb92-16"></a>        </span>
<span id="cb92-17"><a href="regularized-logistic-regression.html#cb92-17"></a>        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>degree) {</span>
<span id="cb92-18"><a href="regularized-logistic-regression.html#cb92-18"></a>                <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>i) {</span>
<span id="cb92-19"><a href="regularized-logistic-regression.html#cb92-19"></a>                        <span class="co">#browser() - was used to debug your function</span></span>
<span id="cb92-20"><a href="regularized-logistic-regression.html#cb92-20"></a>                        out &lt;-<span class="st"> </span><span class="kw">cbind</span>(out, (X1<span class="op">^</span>(i<span class="op">-</span>j))<span class="op">*</span>(X2<span class="op">^</span>j))</span>
<span id="cb92-21"><a href="regularized-logistic-regression.html#cb92-21"></a>                }</span>
<span id="cb92-22"><a href="regularized-logistic-regression.html#cb92-22"></a>        }</span>
<span id="cb92-23"><a href="regularized-logistic-regression.html#cb92-23"></a>        <span class="kw">return</span>(out)  </span>
<span id="cb92-24"><a href="regularized-logistic-regression.html#cb92-24"></a>}</span>
<span id="cb92-25"><a href="regularized-logistic-regression.html#cb92-25"></a></span>
<span id="cb92-26"><a href="regularized-logistic-regression.html#cb92-26"></a><span class="co"># This is very very close to the octave file but not exact. Come back to it if you have different thetas.</span></span>
<span id="cb92-27"><a href="regularized-logistic-regression.html#cb92-27"></a>X &lt;-<span class="st"> </span><span class="kw">mapFeature</span>(X[,<span class="dv">2</span>], X[,<span class="dv">3</span>])</span></code></pre></div>
<p>The map feature does what is shown in the following figure.</p>
<p><img src="mapFeature.png" /></p>
</div>
<div id="cost-function-and-gradient-1" class="section level2">
<h2><span class="header-section-number">9.3</span> 2.3 Cost function and gradient</h2>
<p>Now you will implement code to compute the cost function and gradient for regularized logistic regression. Complete the code in costFunctionReg.m to return the cost and gradient.</p>
<p>Recall that the regularized cost function in logistic regression is</p>
<p><span class="math inline">\(J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2\)</span></p>
<p>Note that you should not regularize the parameter <span class="math inline">\(\theta_0\)</span>. In Octave/MATLAB, recall that indexing starts from 1, hence, you should not be regularizing the theta(1) parameter (which corresponds to <span class="math inline">\(\theta_0\)</span>) in the code. The gradient of the cost function is a vector where the <span class="math inline">\(j^{th}\)</span> element is defined as follows:</p>
<p><span class="math display">\[\begin{align} &amp; \text{Repeat}\ \lbrace \newline &amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline &amp; \rbrace \end{align}\]</span></p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="regularized-logistic-regression.html#cb93-1"></a><span class="co"># Regularized cost and gradient </span></span>
<span id="cb93-2"><a href="regularized-logistic-regression.html#cb93-2"></a></span>
<span id="cb93-3"><a href="regularized-logistic-regression.html#cb93-3"></a>m &lt;-<span class="st"> </span><span class="kw">dim</span>(X)[<span class="dv">1</span>]</span>
<span id="cb93-4"><a href="regularized-logistic-regression.html#cb93-4"></a>n &lt;-<span class="st"> </span><span class="kw">dim</span>(X)[<span class="dv">2</span>]</span>
<span id="cb93-5"><a href="regularized-logistic-regression.html#cb93-5"></a></span>
<span id="cb93-6"><a href="regularized-logistic-regression.html#cb93-6"></a><span class="co"># 5. Set initial theta&#39;s </span></span>
<span id="cb93-7"><a href="regularized-logistic-regression.html#cb93-7"></a></span>
<span id="cb93-8"><a href="regularized-logistic-regression.html#cb93-8"></a>initialTheta &lt;-<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n), n, <span class="dv">1</span>)</span>
<span id="cb93-9"><a href="regularized-logistic-regression.html#cb93-9"></a></span>
<span id="cb93-10"><a href="regularized-logistic-regression.html#cb93-10"></a></span>
<span id="cb93-11"><a href="regularized-logistic-regression.html#cb93-11"></a><span class="co"># 6. Make the cost function</span></span>
<span id="cb93-12"><a href="regularized-logistic-regression.html#cb93-12"></a><span class="co">#       - Input (Initial theta, X, y, lambda)</span></span>
<span id="cb93-13"><a href="regularized-logistic-regression.html#cb93-13"></a><span class="co">#       - Output (Cost and gradient)</span></span>
<span id="cb93-14"><a href="regularized-logistic-regression.html#cb93-14"></a></span>
<span id="cb93-15"><a href="regularized-logistic-regression.html#cb93-15"></a><span class="co"># Important to regularize the first parameter both in the grad and the cost.</span></span>
<span id="cb93-16"><a href="regularized-logistic-regression.html#cb93-16"></a><span class="co"># Main trick done here is to replace the sum function with a vector multiplication</span></span>
<span id="cb93-17"><a href="regularized-logistic-regression.html#cb93-17"></a><span class="co"># by creating a row vector of 1. Then to not regularize the first theta replace it with 0 </span></span>
<span id="cb93-18"><a href="regularized-logistic-regression.html#cb93-18"></a><span class="co"># and take theta values starting from the second to the end.</span></span>
<span id="cb93-19"><a href="regularized-logistic-regression.html#cb93-19"></a></span>
<span id="cb93-20"><a href="regularized-logistic-regression.html#cb93-20"></a>costFunctionReg &lt;-<span class="st"> </span><span class="cf">function</span>(theta, X, y, lambda){</span>
<span id="cb93-21"><a href="regularized-logistic-regression.html#cb93-21"></a>        cost &lt;-<span class="st">  </span><span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>((<span class="kw">t</span>(<span class="op">-</span>y)<span class="op">%*%</span><span class="kw">log</span>(<span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta))<span class="op">-</span></span>
<span id="cb93-22"><a href="regularized-logistic-regression.html#cb93-22"></a><span class="st">                        </span><span class="kw">t</span>((<span class="dv">1</span><span class="op">-</span>y))<span class="op">%*%</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span><span class="st"> </span><span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta)))) <span class="op">+</span><span class="st"> </span></span>
<span id="cb93-23"><a href="regularized-logistic-regression.html#cb93-23"></a><span class="st">          </span>( (lambda<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span><span class="st"> </span>(<span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta)) <span class="op">%*%</span><span class="st"> </span>(<span class="kw">c</span>(<span class="dv">0</span>, theta[<span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(theta)]))<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb93-24"><a href="regularized-logistic-regression.html#cb93-24"></a>        </span>
<span id="cb93-25"><a href="regularized-logistic-regression.html#cb93-25"></a>        grad &lt;-<span class="st">  </span><span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>(<span class="kw">t</span>(<span class="kw">sigmoid</span>(X <span class="op">%*%</span><span class="st"> </span>theta) <span class="op">-</span><span class="st"> </span>y)<span class="op">%*%</span>X) <span class="op">+</span><span class="st"> </span>((lambda<span class="op">/</span>m)<span class="op">*</span>(<span class="kw">c</span>(<span class="dv">0</span>, theta[<span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(theta)])))</span>
<span id="cb93-26"><a href="regularized-logistic-regression.html#cb93-26"></a>        <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">cost =</span> cost, <span class="dt">grad =</span> grad))</span>
<span id="cb93-27"><a href="regularized-logistic-regression.html#cb93-27"></a>}</span>
<span id="cb93-28"><a href="regularized-logistic-regression.html#cb93-28"></a></span>
<span id="cb93-29"><a href="regularized-logistic-regression.html#cb93-29"></a></span>
<span id="cb93-30"><a href="regularized-logistic-regression.html#cb93-30"></a><span class="co"># Expected gradients with initial theta - actually from octave for accuracy (0.008475, 0.018788, 0.000078, 0.050345, 0.011501) - Expected cost - 0.693147</span></span>
<span id="cb93-31"><a href="regularized-logistic-regression.html#cb93-31"></a><span class="kw">costFunctionReg</span>(initialTheta, X, y, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## $cost
##           [,1]
## [1,] 0.6931472
## 
## $grad
##             [,1]       [,2]         [,3]       [,4]       [,5]       [,6]
## [1,] 0.008474576 0.01878809 7.777119e-05 0.05034464 0.01150133 0.03766485
##            [,7]        [,8]        [,9]      [,10]      [,11]       [,12]
## [1,] 0.01835599 0.007323934 0.008192445 0.02347649 0.03934862 0.002239239
##           [,13]       [,14]      [,15]      [,16]       [,17]       [,18]
## [1,] 0.01286005 0.003095937 0.03930282 0.01997075 0.004329832 0.003386439
##            [,19]       [,20]      [,21]      [,22]       [,23]       [,24]
## [1,] 0.005838221 0.004476291 0.03100798 0.03103124 0.001097402 0.006315708
##            [,25]       [,26]       [,27]      [,28]
## [1,] 0.000408503 0.007265043 0.001376462 0.03879364</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="regularized-logistic-regression.html#cb95-1"></a>testTheta &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), n, <span class="dv">1</span>)</span>
<span id="cb95-2"><a href="regularized-logistic-regression.html#cb95-2"></a><span class="co"># expected cost - 3.164509 &amp; expected thetas for the first 5 gradients (0.346045, 0.161352, 0.194796,  0.226863, 0.092186)</span></span>
<span id="cb95-3"><a href="regularized-logistic-regression.html#cb95-3"></a><span class="kw">costFunctionReg</span>(testTheta, X, y, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## $cost
##          [,1]
## [1,] 3.164509
## 
## $grad
##           [,1]      [,2]      [,3]      [,4]       [,5]      [,6]      [,7]
## [1,] 0.3460451 0.1613519 0.1947958 0.2268628 0.09218568 0.2443856 0.1433921
##           [,8]      [,9]     [,10]   [,11]      [,12]     [,13]     [,14]
## [1,] 0.1084417 0.1023144 0.1834685 0.17353 0.08725552 0.1182278 0.0858433
##          [,15]     [,16]      [,17]      [,18]      [,19]      [,20]     [,21]
## [1,] 0.1999489 0.1352265 0.09497527 0.09356441 0.09979784 0.09140157 0.1748524
##          [,22]      [,23]      [,24]      [,25]     [,26]      [,27]     [,28]
## [1,] 0.1495544 0.08678566 0.09897686 0.08531951 0.1019067 0.08450198 0.1822832</code></pre>
</div>
<div id="learning-parameters-using-fminunc-1" class="section level2">
<h2><span class="header-section-number">9.4</span> 2.4 Learning parameters using fminunc</h2>
<p>Similar to the previous parts, you will use fminunc to learn the optimal parameters θ. If you have completed the cost and gradient for regularized logistic regression (costFunctionReg.m) correctly, you should be able to step through the next part of ex2 reg.m to learn the parameters θ using fminunc.</p>
<p><span style="color:red">
Correction: objective and grad function that is passed to the fminunc call can take more than one variable. However, the variable to be optimized should be the first argument and specific value for the other variables should be passed to the advanced minimization algorithm after the objective and the gradient functions.
</span></p>
<p><span style="color:red">
There is also a small difference in the gradients the reason for which I did not understand at the moment.
</span></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="regularized-logistic-regression.html#cb97-1"></a>costReg &lt;-<span class="st">  </span><span class="cf">function</span>(theta, X, y, lambda){</span>
<span id="cb97-2"><a href="regularized-logistic-regression.html#cb97-2"></a>  <span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>((<span class="kw">t</span>(<span class="op">-</span>y)<span class="op">%*%</span><span class="kw">log</span>(<span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta))<span class="op">-</span></span>
<span id="cb97-3"><a href="regularized-logistic-regression.html#cb97-3"></a><span class="st">                        </span><span class="kw">t</span>((<span class="dv">1</span><span class="op">-</span>y))<span class="op">%*%</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span><span class="st"> </span><span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta)))) <span class="op">+</span><span class="st"> </span></span>
<span id="cb97-4"><a href="regularized-logistic-regression.html#cb97-4"></a><span class="st">          </span>( (lambda<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span><span class="st"> </span>(<span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta)) <span class="op">%*%</span><span class="st"> </span>(<span class="kw">c</span>(<span class="dv">0</span>, theta[<span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(theta)]))<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb97-5"><a href="regularized-logistic-regression.html#cb97-5"></a>}</span>
<span id="cb97-6"><a href="regularized-logistic-regression.html#cb97-6"></a>        </span>
<span id="cb97-7"><a href="regularized-logistic-regression.html#cb97-7"></a>gradReg &lt;-<span class="st">  </span><span class="cf">function</span>(theta, X, y, lambda){</span>
<span id="cb97-8"><a href="regularized-logistic-regression.html#cb97-8"></a>  <span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>(<span class="kw">t</span>(<span class="kw">sigmoid</span>(X <span class="op">%*%</span><span class="st"> </span>theta) <span class="op">-</span><span class="st"> </span>y)<span class="op">%*%</span>X) <span class="op">+</span><span class="st"> </span>((lambda<span class="op">/</span>m)<span class="op">*</span>(<span class="kw">c</span>(<span class="dv">0</span>, theta[<span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(theta)])))</span>
<span id="cb97-9"><a href="regularized-logistic-regression.html#cb97-9"></a>}</span>
<span id="cb97-10"><a href="regularized-logistic-regression.html#cb97-10"></a></span>
<span id="cb97-11"><a href="regularized-logistic-regression.html#cb97-11"></a></span>
<span id="cb97-12"><a href="regularized-logistic-regression.html#cb97-12"></a><span class="co"># Expected cost - J =  0.52900 and first 5 gradients  (1.273005, 0.624876, 1.177376, -2.020142, -0.912616)</span></span>
<span id="cb97-13"><a href="regularized-logistic-regression.html#cb97-13"></a>regOptim &lt;-<span class="st"> </span>ucminf<span class="op">::</span><span class="kw">ucminf</span>(<span class="dt">par =</span> initialTheta, <span class="dt">fn =</span> costReg, <span class="dt">gr =</span> gradReg,  <span class="dt">X =</span> X, <span class="dt">y =</span>y, <span class="dt">lambda =</span> <span class="dv">1</span>, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">trace =</span> <span class="dv">0</span>, <span class="dt">maxeval=</span> <span class="dv">400</span>))</span>
<span id="cb97-14"><a href="regularized-logistic-regression.html#cb97-14"></a></span>
<span id="cb97-15"><a href="regularized-logistic-regression.html#cb97-15"></a>regOptim[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>)]</span></code></pre></div>
<pre><code>## $value
## [1] 0.5290027
## 
## $par
##  [1]  1.27273642  0.62521798  1.18107445 -2.01998642 -0.91741042 -1.43161497
##  [7]  0.12409395 -0.36549642 -0.35723318 -0.17514450 -1.45818376 -0.05097913
## [13] -0.61562760 -0.27471004 -1.19279589 -0.24211333 -0.20596939 -0.04474605
## [19] -0.27780142 -0.29536995 -0.45637850 -1.04321107  0.02778775 -0.29246052
## [25]  0.01555863 -0.32740993 -0.14389754 -0.92465253</code></pre>
</div>
<div id="plotting-the-decision-boundary" class="section level2">
<h2><span class="header-section-number">9.5</span> 2.5 Plotting the decision boundary</h2>
<p>To help you visualize the model learned by this classifier, we have provided the function plotDecisionBoundary.m which plots the (non-linear) decision boundary that separates the positive and negative examples. In plotDecisionBoundary.m, we plot the non-linear decision boundary by computing the classifier’s predictions on an evenly spaced grid and then and drew a contour plot of where the predictions change from y = 0 to y = 1.</p>
<p>After learning the parameters θ, the next step in ex reg.m will plot a decision boundary similar to Figure 4.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="regularized-logistic-regression.html#cb99-1"></a><span class="co"># # Plot both groups - X should include intercept</span></span>
<span id="cb99-2"><a href="regularized-logistic-regression.html#cb99-2"></a>plotDecisionBoundary &lt;-<span class="st"> </span><span class="cf">function</span>(X, y, <span class="dt">lambda=</span><span class="ot">NULL</span>, initialTheta){</span>
<span id="cb99-3"><a href="regularized-logistic-regression.html#cb99-3"></a>  <span class="cf">if</span> (<span class="kw">dim</span>(X)[<span class="dv">2</span>]<span class="op">&lt;=</span><span class="st"> </span><span class="dv">3</span>) {</span>
<span id="cb99-4"><a href="regularized-logistic-regression.html#cb99-4"></a>    finalTheta &lt;-<span class="st"> </span>ucminf<span class="op">::</span><span class="kw">ucminf</span>(<span class="dt">par =</span> initialTheta, <span class="dt">fn =</span> costReg, <span class="dt">gr =</span> gradReg,  <span class="dt">X =</span> X, <span class="dt">y =</span>y, <span class="dt">lambda =</span> lambda, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">trace =</span> <span class="dv">0</span>, <span class="dt">maxeval=</span> <span class="dv">400</span>))<span class="op">$</span>par</span>
<span id="cb99-5"><a href="regularized-logistic-regression.html#cb99-5"></a></span>
<span id="cb99-6"><a href="regularized-logistic-regression.html#cb99-6"></a>    pch.list &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb99-7"><a href="regularized-logistic-regression.html#cb99-7"></a>    pch.list[ex2data1<span class="op">$</span>admission <span class="op">==</span><span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="dv">21</span></span>
<span id="cb99-8"><a href="regularized-logistic-regression.html#cb99-8"></a>    pch.list[ex2data1<span class="op">$</span>admission <span class="op">==</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">43</span></span>
<span id="cb99-9"><a href="regularized-logistic-regression.html#cb99-9"></a>    </span>
<span id="cb99-10"><a href="regularized-logistic-regression.html#cb99-10"></a></span>
<span id="cb99-11"><a href="regularized-logistic-regression.html#cb99-11"></a>    <span class="kw">plot</span>(examScore2 <span class="op">~</span><span class="st"> </span>examScore1, ex2data1, <span class="dt">ylim=</span><span class="kw">range</span>(<span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">100</span>)), <span class="dt">xlim=</span><span class="kw">range</span>(<span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">100</span>)), </span>
<span id="cb99-12"><a href="regularized-logistic-regression.html#cb99-12"></a>       <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">pch =</span> <span class="kw">c</span>(pch.list), <span class="dt">bg =</span> <span class="st">&quot;yellow&quot;</span>)</span>
<span id="cb99-13"><a href="regularized-logistic-regression.html#cb99-13"></a></span>
<span id="cb99-14"><a href="regularized-logistic-regression.html#cb99-14"></a>    <span class="co"># Add legend to top right, outside plot region</span></span>
<span id="cb99-15"><a href="regularized-logistic-regression.html#cb99-15"></a>    <span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">inset=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="op">-</span><span class="fl">0.0375</span>), <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Not admitted&quot;</span>,<span class="st">&quot;Admitted&quot;</span>, <span class="st">&quot;Decision Boundary&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;yellow&quot;</span>,<span class="dv">1</span>,<span class="dv">3</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>, <span class="dv">43</span>, <span class="dv">95</span>))</span>
<span id="cb99-16"><a href="regularized-logistic-regression.html#cb99-16"></a>    <span class="kw">abline</span>(<span class="dt">a =</span> <span class="op">-</span>finalTheta[<span class="dv">1</span>]<span class="op">/</span>finalTheta[<span class="dv">3</span>], <span class="dt">b =</span> <span class="op">-</span>finalTheta[<span class="dv">2</span>]<span class="op">/</span>finalTheta[<span class="dv">3</span>], <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>)</span>
<span id="cb99-17"><a href="regularized-logistic-regression.html#cb99-17"></a>  } <span class="cf">else</span> {</span>
<span id="cb99-18"><a href="regularized-logistic-regression.html#cb99-18"></a>    regOptim &lt;-<span class="st"> </span>ucminf<span class="op">::</span><span class="kw">ucminf</span>(<span class="dt">par =</span> initialTheta, <span class="dt">fn =</span> costReg, <span class="dt">gr =</span> gradReg,  <span class="dt">X =</span> X, <span class="dt">y =</span>y, <span class="dt">lambda =</span> lambda, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">trace =</span> <span class="dv">0</span>, <span class="dt">maxeval=</span> <span class="dv">400</span>))</span>
<span id="cb99-19"><a href="regularized-logistic-regression.html#cb99-19"></a>    </span>
<span id="cb99-20"><a href="regularized-logistic-regression.html#cb99-20"></a>    u &lt;-<span class="st">  </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dt">length =</span> <span class="dv">50</span>)</span>
<span id="cb99-21"><a href="regularized-logistic-regression.html#cb99-21"></a>    v &lt;-<span class="st">  </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dt">length =</span> <span class="dv">50</span>)</span>
<span id="cb99-22"><a href="regularized-logistic-regression.html#cb99-22"></a></span>
<span id="cb99-23"><a href="regularized-logistic-regression.html#cb99-23"></a>    z &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="kw">length</span>(u), <span class="kw">length</span>(v))</span>
<span id="cb99-24"><a href="regularized-logistic-regression.html#cb99-24"></a></span>
<span id="cb99-25"><a href="regularized-logistic-regression.html#cb99-25"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(u)) {</span>
<span id="cb99-26"><a href="regularized-logistic-regression.html#cb99-26"></a>        <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(v)) {</span>
<span id="cb99-27"><a href="regularized-logistic-regression.html#cb99-27"></a>                z[i,j] &lt;-<span class="st"> </span><span class="kw">mapFeature</span>(u[i],v[j])<span class="op">%*%</span>regOptim<span class="op">$</span>par</span>
<span id="cb99-28"><a href="regularized-logistic-regression.html#cb99-28"></a>        }</span>
<span id="cb99-29"><a href="regularized-logistic-regression.html#cb99-29"></a>    }</span>
<span id="cb99-30"><a href="regularized-logistic-regression.html#cb99-30"></a>    pch.list &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb99-31"><a href="regularized-logistic-regression.html#cb99-31"></a>    pch.list[ex2data2<span class="op">$</span>qualityAssurance <span class="op">==</span><span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="dv">21</span></span>
<span id="cb99-32"><a href="regularized-logistic-regression.html#cb99-32"></a>    pch.list[ex2data2<span class="op">$</span>qualityAssurance <span class="op">==</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">43</span></span>
<span id="cb99-33"><a href="regularized-logistic-regression.html#cb99-33"></a></span>
<span id="cb99-34"><a href="regularized-logistic-regression.html#cb99-34"></a>    <span class="kw">plot</span>(microChip2 <span class="op">~</span><span class="st"> </span>microChip1, ex2data2, <span class="dt">ylim=</span><span class="kw">range</span>(<span class="kw">c</span>(X[,<span class="dv">3</span>], X[,<span class="dv">3</span>]<span class="op">+</span><span class="fl">0.1</span>)), <span class="dt">xlim=</span><span class="kw">range</span>(<span class="kw">c</span>(X[,<span class="dv">2</span>], X[,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.25</span>)),</span>
<span id="cb99-35"><a href="regularized-logistic-regression.html#cb99-35"></a>       <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">pch =</span> <span class="kw">c</span>(pch.list),<span class="dt">bg=</span> <span class="st">&quot;yellow&quot;</span>, <span class="dt">axes =</span> <span class="ot">FALSE</span>)</span>
<span id="cb99-36"><a href="regularized-logistic-regression.html#cb99-36"></a>    <span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">at=</span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dt">by =</span> <span class="fl">0.5</span> ))</span>
<span id="cb99-37"><a href="regularized-logistic-regression.html#cb99-37"></a>    <span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">2</span>, <span class="dt">at=</span><span class="kw">seq</span>(<span class="op">-</span><span class="fl">0.8</span>, <span class="fl">1.2</span>, <span class="dt">by=</span><span class="fl">0.2</span>))</span>
<span id="cb99-38"><a href="regularized-logistic-regression.html#cb99-38"></a></span>
<span id="cb99-39"><a href="regularized-logistic-regression.html#cb99-39"></a>    <span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">inset=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;y = 0&quot;</span>,<span class="st">&quot;y = 1&quot;</span>, <span class="st">&quot;decision boudnary&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;yellow&quot;</span>, <span class="dv">1</span>, <span class="dv">3</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>, <span class="dv">43</span>, <span class="dv">95</span>))</span>
<span id="cb99-40"><a href="regularized-logistic-regression.html#cb99-40"></a>    <span class="kw">contour</span>(<span class="dt">x =</span> u, <span class="dt">y =</span> v, <span class="dt">z =</span> z, <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>)</span>
<span id="cb99-41"><a href="regularized-logistic-regression.html#cb99-41"></a>  }</span>
<span id="cb99-42"><a href="regularized-logistic-regression.html#cb99-42"></a>}</span>
<span id="cb99-43"><a href="regularized-logistic-regression.html#cb99-43"></a></span>
<span id="cb99-44"><a href="regularized-logistic-regression.html#cb99-44"></a><span class="co"># This would be the previous plot --- reason why you should develop more general functions</span></span>
<span id="cb99-45"><a href="regularized-logistic-regression.html#cb99-45"></a><span class="co"># plotDecisionBoundary(X = cbind(ones = rep(1,dim(ex2data1)[1]),as.matrix(ex2data1[,1:2])), y = as.numeric(as.matrix(ex2data1[,3])), initialTheta = matrix(rep(0, 3), 3, 1), lambda = 0)</span></span>
<span id="cb99-46"><a href="regularized-logistic-regression.html#cb99-46"></a></span>
<span id="cb99-47"><a href="regularized-logistic-regression.html#cb99-47"></a><span class="kw">plotDecisionBoundary</span>(<span class="dt">X =</span> X, <span class="dt">y =</span> y, <span class="dt">lambda =</span> <span class="dv">1</span>, <span class="dt">initialTheta =</span> initialTheta)</span></code></pre></div>
<p><img src="bookdw_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
</div>
<div id="varying-lambda-levels" class="section level2">
<h2><span class="header-section-number">9.6</span> 2.4 Varying lambda levels</h2>
<p>In this part of the exercise, you will get to try out different regularization parameters for the dataset to understand how regularization prevents overfitting.</p>
<p>Notice the changes in the decision boundary as you vary λ. With a small λ, you should find that the classifier gets almost every training example correct, but draws a very complicated boundary, thus overfitting the data (Figure 5). This is not a good decision boundary: for example, it predicts that a point at x = (−0.25, 1.5) is accepted (y = 1), which seems to be an
incorrect decision given the training set.</p>
<p>With a larger λ, you should see a plot that shows an simpler decision boundary which still separates the positives and negatives fairly well. However, if λ is set to too high a value, you will not get a good fit and the decision boundary will not follow the data so well, thus underfitting the data (Figure 6).</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="regularized-logistic-regression.html#cb100-1"></a><span class="co"># The height setting was a bit annoying. So you have to set the plotOut height as auto and then create a function inside render plot to set it to a value you want.</span></span>
<span id="cb100-2"><a href="regularized-logistic-regression.html#cb100-2"></a><span class="kw">renderPlot</span>({</span>
<span id="cb100-3"><a href="regularized-logistic-regression.html#cb100-3"></a>  <span class="kw">plotOutput</span>(<span class="kw">plotDecisionBoundary</span>(<span class="dt">X =</span> X, <span class="dt">y =</span> y, <span class="dt">lambda =</span> input<span class="op">$</span>lambda, <span class="dt">initialTheta =</span> initialTheta), <span class="dt">height =</span> <span class="st">&quot;auto&quot;</span>)</span>
<span id="cb100-4"><a href="regularized-logistic-regression.html#cb100-4"></a>}, <span class="dt">height =</span> <span class="cf">function</span>() {</span>
<span id="cb100-5"><a href="regularized-logistic-regression.html#cb100-5"></a>      <span class="dv">600</span></span>
<span id="cb100-6"><a href="regularized-logistic-regression.html#cb100-6"></a>  })</span></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ml-ex-2-r-implementationa-logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ml-programming-exercise-3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdw.pdf", "bookdw.epub"]
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

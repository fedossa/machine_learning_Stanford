<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 1 ML Ex 2 R implementationa: Logistic Regression | Machine Learning Notes - Andrew Ng</title>
  <meta name="description" content="Study material for ML - Stanford University" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="9 1 ML Ex 2 R implementationa: Logistic Regression | Machine Learning Notes - Andrew Ng" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Study material for ML - Stanford University" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 1 ML Ex 2 R implementationa: Logistic Regression | Machine Learning Notes - Andrew Ng" />
  
  <meta name="twitter:description" content="Study material for ML - Stanford University" />
  

<meta name="author" content="Fikir Worku Edossa" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="what-happens-here.html"/>
<link rel="next" href="regularized-logistic-regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="week-1.html"><a href="week-1.html"><i class="fa fa-check"></i><b>2</b> Week 1</a><ul>
<li class="chapter" data-level="2.1" data-path="week-1.html"><a href="week-1.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="2.2" data-path="week-1.html"><a href="week-1.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="2.2.1" data-path="week-1.html"><a href="week-1.html#supervised-learning-note"><i class="fa fa-check"></i><b>2.2.1</b> Supervised Learning (note)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="week-1.html"><a href="week-1.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="2.4" data-path="week-1.html"><a href="week-1.html#model-representation"><i class="fa fa-check"></i><b>2.4</b> Model Representation</a></li>
<li class="chapter" data-level="2.5" data-path="week-1.html"><a href="week-1.html#cost-function"><i class="fa fa-check"></i><b>2.5</b> Cost Function</a></li>
<li class="chapter" data-level="2.6" data-path="week-1.html"><a href="week-1.html#cost-function---intution-i"><i class="fa fa-check"></i><b>2.6</b> Cost Function - Intution I</a></li>
<li class="chapter" data-level="2.7" data-path="week-1.html"><a href="week-1.html#cost-function---intution-ii"><i class="fa fa-check"></i><b>2.7</b> Cost Function - Intution II</a></li>
<li class="chapter" data-level="2.8" data-path="week-1.html"><a href="week-1.html#gradient-descent"><i class="fa fa-check"></i><b>2.8</b> Gradient Descent</a></li>
<li class="chapter" data-level="2.9" data-path="week-1.html"><a href="week-1.html#gradient-descent-intution"><i class="fa fa-check"></i><b>2.9</b> Gradient Descent Intution</a></li>
<li class="chapter" data-level="2.10" data-path="week-1.html"><a href="week-1.html#gradient-descent-for-linear-regression"><i class="fa fa-check"></i><b>2.10</b> Gradient Descent for Linear Regression</a></li>
<li class="chapter" data-level="2.11" data-path="week-1.html"><a href="week-1.html#matrices-and-vectors"><i class="fa fa-check"></i><b>2.11</b> Matrices and Vectors</a></li>
<li class="chapter" data-level="2.12" data-path="week-1.html"><a href="week-1.html#addition-and-scalar-multiplication"><i class="fa fa-check"></i><b>2.12</b> Addition and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.13" data-path="week-1.html"><a href="week-1.html#matrix-vector-multiplication"><i class="fa fa-check"></i><b>2.13</b> Matrix-Vector Multiplication</a></li>
<li class="chapter" data-level="2.14" data-path="week-1.html"><a href="week-1.html#matrix-matrix-multiplication"><i class="fa fa-check"></i><b>2.14</b> Matrix-Matrix Multiplication</a></li>
<li class="chapter" data-level="2.15" data-path="week-1.html"><a href="week-1.html#matrix-multiplication-properties"><i class="fa fa-check"></i><b>2.15</b> Matrix Multiplication Properties</a></li>
<li class="chapter" data-level="2.16" data-path="week-1.html"><a href="week-1.html#inverse-and-transpose"><i class="fa fa-check"></i><b>2.16</b> Inverse and Transpose</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2.html"><a href="week-2.html"><i class="fa fa-check"></i><b>3</b> Week 2</a><ul>
<li class="chapter" data-level="3.1" data-path="week-2.html"><a href="week-2.html#multiple-features"><i class="fa fa-check"></i><b>3.1</b> Multiple features</a></li>
<li class="chapter" data-level="3.2" data-path="week-2.html"><a href="week-2.html#gradient-descent-for-multiple-variables"><i class="fa fa-check"></i><b>3.2</b> Gradient Descent for Multiple Variables</a></li>
<li class="chapter" data-level="3.3" data-path="week-2.html"><a href="week-2.html#gradient-descent-in-practice-i---feature-scaling"><i class="fa fa-check"></i><b>3.3</b> Gradient Descent in Practice I - Feature Scaling</a></li>
<li class="chapter" data-level="3.4" data-path="week-2.html"><a href="week-2.html#gradient-descent-in-practice-ii---learning-rate"><i class="fa fa-check"></i><b>3.4</b> Gradient Descent in Practice II - Learning Rate</a></li>
<li class="chapter" data-level="3.5" data-path="week-2.html"><a href="week-2.html#features-and-polynomial-regression"><i class="fa fa-check"></i><b>3.5</b> Features and Polynomial Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="week-2.html"><a href="week-2.html#polynomial-regression"><i class="fa fa-check"></i><b>3.5.1</b> Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="week-2.html"><a href="week-2.html#normal-equation"><i class="fa fa-check"></i><b>3.6</b> Normal Equation</a></li>
<li class="chapter" data-level="3.7" data-path="week-2.html"><a href="week-2.html#normal-equation-non-invertibility"><i class="fa fa-check"></i><b>3.7</b> Normal Equation Non-invertibility</a></li>
<li class="chapter" data-level="3.8" data-path="week-2.html"><a href="week-2.html#octave---basic-operations"><i class="fa fa-check"></i><b>3.8</b> Octave - Basic Operations</a></li>
<li class="chapter" data-level="3.9" data-path="week-2.html"><a href="week-2.html#moving-data-around"><i class="fa fa-check"></i><b>3.9</b> Moving Data Around</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-3.html"><a href="week-3.html"><i class="fa fa-check"></i><b>4</b> Week 3</a><ul>
<li class="chapter" data-level="4.1" data-path="week-3.html"><a href="week-3.html#classification"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="week-3.html"><a href="week-3.html#logistic-regression---hpothesis-representation"><i class="fa fa-check"></i><b>4.2</b> Logistic regression - Hpothesis Representation</a></li>
<li class="chapter" data-level="4.3" data-path="week-3.html"><a href="week-3.html#decision-boundary"><i class="fa fa-check"></i><b>4.3</b> Decision Boundary</a></li>
<li class="chapter" data-level="4.4" data-path="week-3.html"><a href="week-3.html#logistic-regression---cost-function"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression - Cost Function</a></li>
<li class="chapter" data-level="4.5" data-path="week-3.html"><a href="week-3.html#simplified-cost-functin-and-gradient-descent---logreg"><i class="fa fa-check"></i><b>4.5</b> Simplified Cost Functin and Gradient Descent - LogReg</a><ul>
<li class="chapter" data-level="4.5.1" data-path="week-1.html"><a href="week-1.html#gradient-descent"><i class="fa fa-check"></i><b>4.5.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.5.2" data-path="week-3.html"><a href="week-3.html#partial-derivative-of-jθ"><i class="fa fa-check"></i><b>4.5.2</b> Partial derivative of J(θ)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="week-3.html"><a href="week-3.html#advanced-optimization"><i class="fa fa-check"></i><b>4.6</b> Advanced Optimization</a></li>
<li class="chapter" data-level="4.7" data-path="week-3.html"><a href="week-3.html#multiclass-classification-one-vs-all"><i class="fa fa-check"></i><b>4.7</b> Multiclass Classification: One-vs-all</a></li>
<li class="chapter" data-level="4.8" data-path="week-3.html"><a href="week-3.html#the-problem-of-overfitting"><i class="fa fa-check"></i><b>4.8</b> The Problem of Overfitting</a></li>
<li class="chapter" data-level="4.9" data-path="week-3.html"><a href="week-3.html#regularization-cost-function"><i class="fa fa-check"></i><b>4.9</b> Regularization Cost Function</a></li>
<li class="chapter" data-level="4.10" data-path="week-3.html"><a href="week-3.html#regularized-linear-regression"><i class="fa fa-check"></i><b>4.10</b> Regularized Linear Regression</a><ul>
<li class="chapter" data-level="4.10.1" data-path="week-3.html"><a href="week-3.html#gradient-descent-1"><i class="fa fa-check"></i><b>4.10.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.10.2" data-path="week-2.html"><a href="week-2.html#normal-equation"><i class="fa fa-check"></i><b>4.10.2</b> Normal Equation</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="week-3.html"><a href="week-3.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>4.11</b> Regularized Logistic Regression</a><ul>
<li class="chapter" data-level="4.11.1" data-path="week-1.html"><a href="week-1.html#cost-function"><i class="fa fa-check"></i><b>4.11.1</b> Cost Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-4.html"><a href="week-4.html"><i class="fa fa-check"></i><b>5</b> Week 4</a><ul>
<li class="chapter" data-level="5.1" data-path="week-4.html"><a href="week-4.html#model-representation-i"><i class="fa fa-check"></i><b>5.1</b> Model Representation I</a></li>
<li class="chapter" data-level="5.2" data-path="week-4.html"><a href="week-4.html#model-representation-ii"><i class="fa fa-check"></i><b>5.2</b> Model Representation II</a><ul>
<li class="chapter" data-level="5.2.1" data-path="week-4.html"><a href="week-4.html#examples-and-intuitions-i"><i class="fa fa-check"></i><b>5.2.1</b> Examples and Intuitions I</a></li>
<li class="chapter" data-level="5.2.2" data-path="week-4.html"><a href="week-4.html#examples-and-intuitions-ii"><i class="fa fa-check"></i><b>5.2.2</b> Examples and Intuitions II</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="week-4.html"><a href="week-4.html#multiclass-classification"><i class="fa fa-check"></i><b>5.3</b> Multiclass Classification</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-5.html"><a href="week-5.html"><i class="fa fa-check"></i><b>6</b> Week 5</a><ul>
<li class="chapter" data-level="6.1" data-path="week-5.html"><a href="week-5.html#cost-function-and-backpropagation---neural-networks"><i class="fa fa-check"></i><b>6.1</b> Cost Function and Backpropagation - Neural Networks</a><ul>
<li class="chapter" data-level="6.1.1" data-path="week-1.html"><a href="week-1.html#cost-function"><i class="fa fa-check"></i><b>6.1.1</b> Cost Function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="week-5.html"><a href="week-5.html#backpropagation-algorithm"><i class="fa fa-check"></i><b>6.2</b> Backpropagation Algorithm</a><ul>
<li class="chapter" data-level="6.2.1" data-path="week-5.html"><a href="week-5.html#back-propagation-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> Back propagation Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="week-5.html"><a href="week-5.html#backpropagation-intuition"><i class="fa fa-check"></i><b>6.3</b> Backpropagation Intuition</a></li>
<li class="chapter" data-level="6.4" data-path="week-5.html"><a href="week-5.html#implementation-note-unrolling-parameters"><i class="fa fa-check"></i><b>6.4</b> Implementation Note: Unrolling Parameters</a></li>
<li class="chapter" data-level="6.5" data-path="week-5.html"><a href="week-5.html#gradient-checking"><i class="fa fa-check"></i><b>6.5</b> Gradient Checking</a></li>
<li class="chapter" data-level="6.6" data-path="week-5.html"><a href="week-5.html#random-initialization"><i class="fa fa-check"></i><b>6.6</b> Random Initialization</a></li>
<li class="chapter" data-level="6.7" data-path="week-5.html"><a href="week-5.html#putting-it-together"><i class="fa fa-check"></i><b>6.7</b> Putting it Together</a><ul>
<li class="chapter" data-level="6.7.1" data-path="week-5.html"><a href="week-5.html#training-a-neural-network"><i class="fa fa-check"></i><b>6.7.1</b> Training a Neural Network</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="7" data-path="charlie-chapter.html"><a href="charlie-chapter.html"><i class="fa fa-check"></i><b>7</b> Charlie Chapter</a><ul>
<li class="chapter" data-level="7.1" data-path="charlie-chapter.html"><a href="charlie-chapter.html#including-plots"><i class="fa fa-check"></i><b>7.1</b> Including Plots</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="what-happens-here.html"><a href="what-happens-here.html"><i class="fa fa-check"></i><b>8</b> What happens here?</a></li>
<li class="chapter" data-level="9" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html"><i class="fa fa-check"></i><b>9</b> 1 ML Ex 2 R implementationa: Logistic Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#introduction"><i class="fa fa-check"></i><b>9.1</b> 1.1 Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#data-prep"><i class="fa fa-check"></i><b>9.1.1</b> Data Prep</a></li>
<li class="chapter" data-level="9.1.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#visualizing-the-data"><i class="fa fa-check"></i><b>9.1.2</b> Visualizing the data</a></li>
<li class="chapter" data-level="9.1.3" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#warmup-exercise-sigmoid-function"><i class="fa fa-check"></i><b>9.1.3</b> Warmup exercise: sigmoid function</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cost-function-and-gradient"><i class="fa fa-check"></i><b>9.2</b> 1.2 Cost function and gradient</a></li>
<li class="chapter" data-level="9.3" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#learning-parameters-using-fminunc"><i class="fa fa-check"></i><b>9.3</b> 1.3 Learning parameters using fminunc</a></li>
<li class="chapter" data-level="9.4" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#plot-the-decision-boundary"><i class="fa fa-check"></i><b>9.4</b> 1.4 Plot the decision boundary</a></li>
<li class="chapter" data-level="9.5" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#evaluating-logistic-regression"><i class="fa fa-check"></i><b>9.5</b> 1.5 Evaluating logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-3.html"><a href="week-3.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>10</b> 2 Regularized logistic regression</a><ul>
<li class="chapter" data-level="10.1" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html"><i class="fa fa-check"></i><b>10.1</b> 2.1 Visualizing the data</a></li>
<li class="chapter" data-level="10.2" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#feature-mapping"><i class="fa fa-check"></i><b>10.2</b> 2.2 Feature mapping</a></li>
<li class="chapter" data-level="10.3" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#cost-function-and-gradient-1"><i class="fa fa-check"></i><b>10.3</b> 2.3 Cost function and gradient</a></li>
<li class="chapter" data-level="10.4" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#learning-parameters-using-fminunc-1"><i class="fa fa-check"></i><b>10.4</b> 2.4 Learning parameters using fminunc</a></li>
<li class="chapter" data-level="10.5" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#plotting-the-decision-boundary"><i class="fa fa-check"></i><b>10.5</b> 2.5 Plotting the decision boundary</a></li>
<li class="chapter" data-level="10.6" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#varying-lambda-levels"><i class="fa fa-check"></i><b>10.6</b> 2.4 Varying lambda levels</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html"><i class="fa fa-check"></i><b>11</b> ML - Programming Exercise 3</a><ul>
<li class="chapter" data-level="11.1" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html#introduction-multi-class-classification"><i class="fa fa-check"></i><b>11.1</b> Introduction: Multi-class Classification</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html#dataset"><i class="fa fa-check"></i><b>11.1.1</b> 1.1 Dataset</a></li>
<li class="chapter" data-level="11.1.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#visualizing-the-data"><i class="fa fa-check"></i><b>11.1.2</b> 1.2 Visualizing the data</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Notes - Andrew Ng</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ml-ex-2-r-implementationa-logistic-regression" class="section level1">
<h1><span class="header-section-number">9</span> 1 ML Ex 2 R implementationa: Logistic Regression</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">9.1</span> 1.1 Introduction</h2>
<p>In this exercise, you will implement logistic regression and apply it to two different datasets. In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.</p>
<p>Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams. This outline and the framework code in ex2.m will guide you through the exercise.</p>
<div id="data-prep" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Data Prep</h3>
<p>Because this particular implementation is performed through R. the base code that is provided in ex2.m will not execute. Hence, the data will first have to be loaded.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb66-1"></a><span class="kw">library</span>(readr)</span>
<span id="cb66-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb66-2"></a>ex2data1 &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;ex2data1.txt&quot;</span>, </span>
<span id="cb66-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb66-3"></a>                     <span class="dt">col_names =</span>  <span class="kw">c</span>(<span class="st">&quot;examScore1&quot;</span>, <span class="st">&quot;examScore2&quot;</span>, <span class="st">&quot;admission&quot;</span>),</span>
<span id="cb66-4"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb66-4"></a>                     <span class="dt">col_types =</span> <span class="kw">cols</span>(<span class="dt">admission =</span> <span class="kw">col_factor</span>(<span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;0&quot;</span>, <span class="st">&quot;1&quot;</span>))))</span>
<span id="cb66-5"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb66-5"></a></span>
<span id="cb66-6"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb66-6"></a>X  &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(ex2data1[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>])</span>
<span id="cb66-7"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb66-7"></a>y &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.matrix</span>(ex2data1[,<span class="dv">3</span>]))</span></code></pre></div>
</div>
<div id="visualizing-the-data" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Visualizing the data</h3>
<p>Before starting to implement any learning algorithm, it is always good to visualize the data if possible. In the first part of ex2.m, the code will load the data and display it on a 2-dimensional plot by calling the function plotData. You will now complete the code in plotData so that it displays a figure like Figure 1, where the axes are the two exam scores, and the positive and negative examples are shown with different markers.</p>
<p><img src="ex2plot1.png" /></p>
<p>To help you get more familiar with plotting, we have left plotData.m empty so you can try to implement it yourself. However, this is an optional (ungraded) exercise. We also provide our implementation below so you can copy it or refer to it. If you choose to copy our example, make sure you learn what each of its commands is doing by consulting the Octave/MATLAB documentation.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-1"></a><span class="co">#par(mar=c(4.1, 5.1, 1.1, 8.1), xpd=TRUE)</span></span>
<span id="cb67-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-2"></a></span>
<span id="cb67-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-3"></a><span class="co"># Plot both groups</span></span>
<span id="cb67-4"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-4"></a>pch.list &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb67-5"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-5"></a>pch.list[ex2data1<span class="op">$</span>admission <span class="op">==</span><span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="dv">21</span></span>
<span id="cb67-6"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-6"></a>pch.list[ex2data1<span class="op">$</span>admission <span class="op">==</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">43</span></span>
<span id="cb67-7"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-7"></a></span>
<span id="cb67-8"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-8"></a><span class="kw">plot</span>(examScore2 <span class="op">~</span><span class="st"> </span>examScore1, ex2data1, <span class="dt">ylim=</span><span class="kw">range</span>(<span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">100</span>)), <span class="dt">xlim=</span><span class="kw">range</span>(<span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">100</span>)), </span>
<span id="cb67-9"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-9"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">pch =</span> <span class="kw">c</span>(pch.list),<span class="dt">bg=</span> <span class="st">&quot;yellow&quot;</span>)</span>
<span id="cb67-10"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-10"></a></span>
<span id="cb67-11"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-11"></a><span class="co"># Add legend to top right, outside plot region</span></span>
<span id="cb67-12"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb67-12"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">inset=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="op">-</span><span class="fl">0.0375</span>), <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Not admitted&quot;</span>,<span class="st">&quot;Admitted&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;yellow&quot;</span>,<span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>, <span class="dv">43</span>))</span></code></pre></div>
<p><img src="bookdw_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
<div id="warmup-exercise-sigmoid-function" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Warmup exercise: sigmoid function</h3>
<p>Before you start with the actual cost function, recall that the logistic regression hypothesis is defined as:
<span class="math inline">\(h_θ(x) = g(θ^T x)\)</span>;</p>
<p>where function g is the sigmoid function. The sigmoid function is defined as:</p>
<p><span class="math inline">\(g(z) = \frac{1}{1 + e^{−z}}\)</span></p>
<p>Your first step is to implement this function in sigmoid.m so it can be called by the rest of your program. When you are finished, try testing a few values by calling sigmoid(x) at the Octave/MATLAB command line. For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating sigmoid(0) should give you exactly 0.5. Your code should also work with vectors and matrices. <strong>For a matrix, your function should perform the sigmoid function on every element.</strong></p>
<p>You can submit your solution for grading by typing submit at the Octave/MATLAB command line. The submission script will prompt you for your login e-mail and submission token and ask you which files you want to submit. You can obtain a submission token from the web page for the assignment.</p>
<p>You should now submit your solutions.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb68-1"></a><span class="co"># Set up a sigmoid function and test it.</span></span>
<span id="cb68-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb68-2"></a></span>
<span id="cb68-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb68-3"></a>sigmoid &lt;-<span class="st"> </span><span class="cf">function</span>(x){</span>
<span id="cb68-4"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb68-4"></a>        <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x))</span>
<span id="cb68-5"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb68-5"></a>        }</span>
<span id="cb68-6"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb68-6"></a><span class="kw">sigmoid</span>(<span class="ot">Inf</span>)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb70-1"></a><span class="kw">sigmoid</span>(<span class="op">-</span><span class="ot">Inf</span>)</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb72-1"></a><span class="kw">sigmoid</span>(<span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 0.5</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb74-1"></a>a &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="op">-</span><span class="ot">Inf</span>, <span class="ot">Inf</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb74-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb74-2"></a></span>
<span id="cb74-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb74-3"></a>a</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,] -Inf    0
## [2,]  Inf    1</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb76-1"></a><span class="kw">sigmoid</span>(a)</span></code></pre></div>
<pre><code>##      [,1]      [,2]
## [1,]    0 0.5000000
## [2,]    1 0.7310586</code></pre>
</div>
</div>
<div id="cost-function-and-gradient" class="section level2">
<h2><span class="header-section-number">9.2</span> 1.2 Cost function and gradient</h2>
<p>Now you will implement the cost function and gradient for logistic regression. Complete the code in costFunction.m to return the cost and gradient.</p>
<p>Recall that the cost function in logistic regression is</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{m} \sum_{i=1}^m \large[ -y^{(i)}\ \log (h_\theta (x^{(i)})) - (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)})) \large]\)</span></p>
<p>A vectorized implementation is:</p>
<p><span style="color:red">
<span class="math display">\[\begin{align} &amp; h = g(X\theta)\newline &amp; J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \end{align}\]</span>
</span></p>
<p>and the gradient of the cost is a vector of the same length as <span class="math inline">\(\theta\)</span> where the <span class="math inline">\(j^{th}\)</span> element <span class="math inline">\((for j = 0, 1, ....,n)\)</span> is defined as follows:</p>
<p>Remember that the general form of gradient descent is:</p>
<p><span class="math display">\[\begin{align}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline &amp; \rbrace\end{align}\]</span></p>
<p>We can work out the derivative part using calculus to get:</p>
<p><span class="math display">\[\begin{align} &amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace \end{align}\]</span></p>
<p>Notice that this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta.</p>
<p>A vectorized implementation is:</p>
<p><span style="color: red">
<span class="math inline">\(\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})\)</span>
</span></p>
<p>The vectorized version;</p>
<p><span class="math inline">\(\nabla J(\theta) = \frac{1}{m} \cdot X^T \cdot \left(g\left(X\cdot\theta\right) - \vec{y}\right)\)</span></p>
<p>Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of <span class="math inline">\(h_θ(x)\)</span>.</p>
<p>Once you are done, ex2.m will call your costFunction using the initial parameters of θ. You should see that the cost is about 0.693. You should now submit your solutions.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-1"></a><span class="co"># 4. Add intercept vector to the X matrix and set m(obs) and n(predictors)</span></span>
<span id="cb78-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-2"></a></span>
<span id="cb78-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-3"></a>X &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dt">ones =</span> <span class="kw">rep</span>(<span class="dv">1</span>,<span class="kw">dim</span>(X)[<span class="dv">1</span>]), X)</span>
<span id="cb78-4"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-4"></a>m &lt;-<span class="st"> </span><span class="kw">dim</span>(X)[<span class="dv">1</span>]</span>
<span id="cb78-5"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-5"></a>n &lt;-<span class="st"> </span><span class="kw">dim</span>(X)[<span class="dv">2</span>]</span>
<span id="cb78-6"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-6"></a></span>
<span id="cb78-7"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-7"></a></span>
<span id="cb78-8"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-8"></a><span class="co"># 5. Set initial theta&#39;s </span></span>
<span id="cb78-9"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-9"></a></span>
<span id="cb78-10"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-10"></a>initialTheta &lt;-<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n), n, <span class="dv">1</span>)</span>
<span id="cb78-11"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-11"></a></span>
<span id="cb78-12"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-12"></a></span>
<span id="cb78-13"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-13"></a><span class="co"># 6. Make the cost function</span></span>
<span id="cb78-14"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-14"></a><span class="co">#       - Input (Initial theta, X, y)</span></span>
<span id="cb78-15"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-15"></a><span class="co">#       - Output (Cost and gradient)</span></span>
<span id="cb78-16"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-16"></a></span>
<span id="cb78-17"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-17"></a></span>
<span id="cb78-18"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-18"></a>costFunction &lt;-<span class="st"> </span><span class="cf">function</span>(theta, X, y){</span>
<span id="cb78-19"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-19"></a>        cost &lt;-<span class="st">  </span><span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>((<span class="kw">t</span>(<span class="op">-</span>y)<span class="op">%*%</span><span class="kw">log</span>(<span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta))<span class="op">-</span></span>
<span id="cb78-20"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-20"></a><span class="st">                        </span><span class="kw">t</span>((<span class="dv">1</span><span class="op">-</span>y))<span class="op">%*%</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span><span class="st"> </span><span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta))))</span>
<span id="cb78-21"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-21"></a>        </span>
<span id="cb78-22"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-22"></a>        grad &lt;-<span class="st">  </span><span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>(<span class="kw">t</span>(<span class="kw">sigmoid</span>(X <span class="op">%*%</span><span class="st"> </span>theta) <span class="op">-</span><span class="st"> </span>y)<span class="op">%*%</span>X)</span>
<span id="cb78-23"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-23"></a>        <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">cost =</span> cost, <span class="dt">grad =</span> grad))</span>
<span id="cb78-24"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-24"></a>}</span>
<span id="cb78-25"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-25"></a></span>
<span id="cb78-26"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-26"></a></span>
<span id="cb78-27"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-27"></a><span class="co"># Expected gradients with initial theta (-0.1, -12.0092, -11.2628)</span></span>
<span id="cb78-28"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb78-28"></a><span class="kw">costFunction</span>(initialTheta, X, y)</span></code></pre></div>
<pre><code>## $cost
##           [,1]
## [1,] 0.6931472
## 
## $grad
##      ones examScore1 examScore2
## [1,] -0.1  -12.00922  -11.26284</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb80-1"></a><span class="co"># With test theta - expected cost 0.218, grad: (0.043, 2.566, 2.647)</span></span>
<span id="cb80-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb80-2"></a><span class="kw">costFunction</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">24</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>), X, y)</span></code></pre></div>
<pre><code>## $cost
##           [,1]
## [1,] 0.2183302
## 
## $grad
##            ones examScore1 examScore2
## [1,] 0.04290299   2.566234   2.646797</code></pre>
<p><span style="color: red">
Fully vectorized cost and grad functions :D
</span></p>
</div>
<div id="learning-parameters-using-fminunc" class="section level2">
<h2><span class="header-section-number">9.3</span> 1.3 Learning parameters using fminunc</h2>
<p>In the previous assignment, you found the optimal parameters of a linear regression model by implementing gradent descent. You wrote a cost function and calculated its gradient, then took a gradient descent step accordingly. This time, instead of taking gradient descent steps, you will use an Octave/-MATLAB built-in function called fminunc.</p>
<p>Octave/MATLAB’s fminunc is an optimization solver that finds the minimum of an unconstrained function. For logistic regression, you want to optimize the cost function J(θ) with parameters θ.</p>
<p>Concretely, you are going to use fminunc to find the best parameters θ for the logistic regression cost function, given a fixed dataset (of X and y values). You will pass to fminunc the following inputs:
- The initial values of the parameters we are trying to optimize.
- A function that, when given the training set and a particular θ, computes the logistic regression cost and gradient with respect to θ for the dataset (X, y)</p>
<p>In ex2.m, we already have code written to call fminunc with the correct arguments.</p>
<p>In this code snippet, we first defined the options to be used with fminunc. Specifically, we set the GradObj option to on, which tells fminunc that our function returns both the cost and the gradient. This allows fminunc to use the gradient when minimizing the function. Furthermore, we set the MaxIter option to 400, so that fminunc will run for at most 400 steps before it terminates.</p>
<p>To specify the actual function we are minimizing, we use a “short-hand” for specifying functions with the @(t) ( costFunction(t, X, y) ) . This creates a function, with argument t, which calls your costFunction. This allows us to wrap the costFunction for use with fminunc.</p>
<p>If you have completed the costFunction correctly, fminunc will converge on the right optimization parameters and return the final values of the cost and θ. Notice that by using fminunc, you did not have to write any loops yourself, or set a learning rate like you did for gradient descent. This is all done by fminunc: you only needed to provide a function calculating the cost and the gradient.</p>
<p>Once fminunc completes, ex2.m will call your costFunction function using the optimal parameters of θ. You should see that the cost is about 0.203.</p>
<p>This final θ value will then be used to plot the decision boundary on the training data, resulting in a figure similar to Figure 2. We also encourage you to look at the code in plotDecisionBoundary.m to see how to plot such a boundary using the θ values.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-1"></a><span class="co"># 8. Find an implementation of advanced gradient descent algortihm fminunc</span></span>
<span id="cb82-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-2"></a></span>
<span id="cb82-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-3"></a>objCost &lt;-<span class="st">  </span><span class="cf">function</span> (theta){</span>
<span id="cb82-4"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-4"></a>                <span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>(sum</span>
<span id="cb82-5"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-5"></a>                       (<span class="op">-</span>y<span class="op">*</span><span class="kw">log</span>(<span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta))<span class="op">-</span></span>
<span id="cb82-6"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-6"></a><span class="st">                                       </span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span><span class="st"> </span><span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta)))</span>
<span id="cb82-7"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-7"></a>                )</span>
<span id="cb82-8"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-8"></a>        }</span>
<span id="cb82-9"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-9"></a>        </span>
<span id="cb82-10"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-10"></a>grad &lt;-<span class="st">  </span><span class="cf">function</span>(theta) {</span>
<span id="cb82-11"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-11"></a>                <span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>(<span class="kw">t</span>(<span class="kw">sigmoid</span>(X <span class="op">%*%</span><span class="st"> </span>theta) <span class="op">-</span><span class="st"> </span>y)<span class="op">%*%</span>X)</span>
<span id="cb82-12"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-12"></a>        }</span>
<span id="cb82-13"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-13"></a></span>
<span id="cb82-14"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-14"></a><span class="kw">library</span>(ucminf)</span>
<span id="cb82-15"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-15"></a></span>
<span id="cb82-16"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-16"></a><span class="co"># 9. Set options for fminunc and optimize with initial theta&#39;s</span></span>
<span id="cb82-17"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-17"></a></span>
<span id="cb82-18"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-18"></a><span class="co"># The only arguments that objective and gradient functions that are passed to the </span></span>
<span id="cb82-19"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-19"></a><span class="co"># advanced algorithm can take is the parameters to be optimized - i.e. theta</span></span>
<span id="cb82-20"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-20"></a></span>
<span id="cb82-21"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-21"></a><span class="co"># You can read more about the specific method.</span></span>
<span id="cb82-22"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-22"></a></span>
<span id="cb82-23"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-23"></a><span class="co"># Expected cost : 0.203 - Expected gredients: -25.161, 0.206, 0.201</span></span>
<span id="cb82-24"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-24"></a></span>
<span id="cb82-25"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb82-25"></a>ucminf<span class="op">::</span><span class="kw">ucminf</span>(<span class="dt">par =</span> initialTheta, <span class="dt">fn =</span> objCost, <span class="dt">gr =</span> grad, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">trace =</span> <span class="dv">0</span>))</span></code></pre></div>
<pre><code>## $par
## [1] -25.1613327   0.2062317   0.2014716
## 
## $value
## [1] 0.2034977
## 
## $convergence
## [1] 1
## 
## $message
## [1] &quot;Stopped by small gradient (grtol).&quot;
## 
## $invhessian.lt
## [1] 3314.2043768  -26.3783748  -26.9990573    0.2247604    0.2016443
## [6]    0.2355011
## 
## $info
##  maxgradient     laststep      stepmax        neval 
## 4.236716e-07 2.095353e-05 3.307500e+00 3.200000e+01</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb84-1"></a><span class="co">#optim(par = initialTheta, fn = objCost, gr = grad, method = &quot;Nelder-Mead&quot;, control = list(trace = 0, maxIte = 400))</span></span>
<span id="cb84-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb84-2"></a><span class="co">#optim(par = c(-24, 0.2, 0.2), fn = objCost, gr = grad, method = &quot;Nelder-Mead&quot;, control = list(trace = 0, maxIte = 400))</span></span>
<span id="cb84-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb84-3"></a><span class="co"># It actually converges to the same thing irrespective of the theta, and whether you have provided gradient function or not.</span></span></code></pre></div>
<p>This final θ value will then be used to plot the decision boundary on the training data, resulting in a figure similar to Figure 2. We also encourage you to look at the code in plotDecisionBoundary.m to see how to plot such a boundary using the θ values.</p>
</div>
<div id="plot-the-decision-boundary" class="section level2">
<h2><span class="header-section-number">9.4</span> 1.4 Plot the decision boundary</h2>
<p>For logistic regression, h = sigmoid(X * theta). This describes the relationship between X, theta, and h. We know theta (from gradient descent). We are given X. So we can compute ‘h’. Now, by definition, the decision boundary is the locus of points where h = 0.5, or equivalently (X * theta) = 0, since the sigmoid(0) is 0.5.</p>
<p>Now we can write out the equation for the case where we have two features and a bias unit, and we write X as <span class="math inline">\([x_0 x_1 x_2x]\)</span> and theta as <span class="math inline">\([\theta_0 \theta_1 \theta_2]\)</span></p>
<p><span class="math inline">\(0 = x_0 \theta_0 + x_1 \theta_1 + x_2 \theta_2\)</span></p>
<p><span class="math inline">\(x_0\)</span> is the bias unit, it is hard-coded to 1.</p>
<p><span class="math inline">\(0 = \theta_0 + x_1 \theta_1 + x_2 \theta_2\)</span></p>
<p>Solve for <span class="math inline">\(x_2\)</span></p>
<p><span class="math inline">\(x_2 = -(\theta_0 + x_1 \theta_1) / \theta_2\)</span></p>
<p>Now, to draw a line, you need two points. So pick two values for <span class="math inline">\(x_1\)</span> - anything near the minimum and maximum of the training set will serve. Compute the corresponding values for <span class="math inline">\(x_2\)</span>, and plot the <span class="math inline">\((x_1 x_2)\)</span>) pairs on the horizontal and vertical axes, then draw a line through them.</p>
<p>This line represents the decision boundary.</p>
<p>This is exactly what the plotDecisionBoundary() function does. <span class="math inline">\(x_2\)</span> is the variable “plot_y”, and <span class="math inline">\(x_1\)</span> is the variable “plot_x”.</p>
<p>“plot_x” also adds some factors of -2 and +2 to the min and max values. This is just to visually center the plot with some margin around the edges.</p>
<p>** In R you need the intercept and slop of this line rather than picking any point. i.e. **</p>
<p><span class="math inline">\(x_2 = -\frac{\theta_0 }{\theta_2} - \frac{\theta_1}{\theta_2}x_1\)</span></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-1"></a>finalTheta &lt;-<span class="st"> </span>ucminf<span class="op">::</span><span class="kw">ucminf</span>(<span class="dt">par =</span> initialTheta, <span class="dt">fn =</span> objCost, <span class="dt">gr =</span> grad, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">trace =</span> <span class="dv">0</span>))<span class="op">$</span>par</span>
<span id="cb85-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-2"></a></span>
<span id="cb85-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-3"></a>pch.list &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb85-4"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-4"></a>pch.list[ex2data1<span class="op">$</span>admission <span class="op">==</span><span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="dv">21</span></span>
<span id="cb85-5"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-5"></a>pch.list[ex2data1<span class="op">$</span>admission <span class="op">==</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">43</span></span>
<span id="cb85-6"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-6"></a></span>
<span id="cb85-7"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-7"></a><span class="kw">plot</span>(examScore2 <span class="op">~</span><span class="st"> </span>examScore1, ex2data1, <span class="dt">ylim=</span><span class="kw">range</span>(<span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">100</span>)), <span class="dt">xlim=</span><span class="kw">range</span>(<span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">100</span>)), </span>
<span id="cb85-8"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-8"></a>     <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">pch =</span> <span class="kw">c</span>(pch.list),<span class="dt">bg=</span> <span class="st">&quot;yellow&quot;</span>)</span>
<span id="cb85-9"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-9"></a></span>
<span id="cb85-10"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-10"></a><span class="co"># Add legend to top right, outside plot region</span></span>
<span id="cb85-11"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-11"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">inset=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="op">-</span><span class="fl">0.0375</span>), <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Not admitted&quot;</span>,<span class="st">&quot;Admitted&quot;</span>, <span class="st">&quot;Decision Boundary&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;yellow&quot;</span>,<span class="dv">1</span>, <span class="dv">3</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>, <span class="dv">43</span>, <span class="dv">95</span>))</span>
<span id="cb85-12"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb85-12"></a><span class="kw">abline</span>(<span class="dt">a =</span> <span class="op">-</span>finalTheta[<span class="dv">1</span>]<span class="op">/</span>finalTheta[<span class="dv">3</span>], <span class="dt">b =</span> <span class="op">-</span>finalTheta[<span class="dv">2</span>]<span class="op">/</span>finalTheta[<span class="dv">3</span>], <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>)</span></code></pre></div>
<p><img src="bookdw_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
</div>
<div id="evaluating-logistic-regression" class="section level2">
<h2><span class="header-section-number">9.5</span> 1.5 Evaluating logistic regression</h2>
<p>After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission
probability of 0.776.</p>
<p>Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. In this part, your task is to complete the code in predict.m. The predict function will produce “1” or “0” predictions given a dataset and a learned parameter vector θ.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb86-1"></a>parameters &lt;-<span class="st"> </span>ucminf<span class="op">::</span><span class="kw">ucminf</span>(<span class="dt">par =</span> initialTheta, <span class="dt">fn =</span> objCost, <span class="dt">gr =</span> grad, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">trace =</span> <span class="dv">0</span>))<span class="op">$</span>par</span>
<span id="cb86-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb86-2"></a>prediction &lt;-<span class="st"> </span><span class="cf">function</span>(theta,x1, x2){</span>
<span id="cb86-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb86-3"></a>        <span class="kw">sigmoid</span>(theta[<span class="dv">1</span>]<span class="op">+</span><span class="st"> </span>theta[<span class="dv">2</span>]<span class="op">*</span>x1 <span class="op">+</span><span class="st"> </span>theta[<span class="dv">3</span>]<span class="op">*</span>x2)</span>
<span id="cb86-4"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb86-4"></a>}</span>
<span id="cb86-5"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb86-5"></a></span>
<span id="cb86-6"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb86-6"></a><span class="co"># Use your new thetas to predict - score 45 and 85 respectively. </span></span>
<span id="cb86-7"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb86-7"></a><span class="kw">prediction</span>(parameters, <span class="dv">45</span>, <span class="dv">85</span>)</span></code></pre></div>
<pre><code>## [1] 0.7762907</code></pre>
<p>After you have completed the code in predict.m, the ex2.m script will proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb88-1"></a>p &lt;-<span class="st"> </span><span class="kw">prediction</span>(parameters,X[,<span class="dv">2</span>], X[,<span class="dv">3</span>])</span>
<span id="cb88-2"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb88-2"></a></span>
<span id="cb88-3"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb88-3"></a>accuracy &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">round</span>(p) <span class="op">==</span><span class="st"> </span>y)<span class="op">*</span><span class="dv">100</span></span>
<span id="cb88-4"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cb88-4"></a>accuracy</span></code></pre></div>
<pre><code>## [1] 89</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="what-happens-here.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularized-logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdw.pdf", "bookdw.epub"]
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

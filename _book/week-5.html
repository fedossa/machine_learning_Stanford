<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Week 5 | Machine Learning Notes - Andrew Ng</title>
  <meta name="description" content="Study material for ML - Stanford University" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Week 5 | Machine Learning Notes - Andrew Ng" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Study material for ML - Stanford University" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Week 5 | Machine Learning Notes - Andrew Ng" />
  
  <meta name="twitter:description" content="Study material for ML - Stanford University" />
  

<meta name="author" content="Fikir Worku Edossa" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-4.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="week-1.html"><a href="week-1.html"><i class="fa fa-check"></i><b>2</b> Week 1</a><ul>
<li class="chapter" data-level="2.1" data-path="week-1.html"><a href="week-1.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="2.2" data-path="week-1.html"><a href="week-1.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="2.2.1" data-path="week-1.html"><a href="week-1.html#supervised-learning-note"><i class="fa fa-check"></i><b>2.2.1</b> Supervised Learning (note)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="week-1.html"><a href="week-1.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="2.4" data-path="week-1.html"><a href="week-1.html#model-representation"><i class="fa fa-check"></i><b>2.4</b> Model Representation</a></li>
<li class="chapter" data-level="2.5" data-path="week-1.html"><a href="week-1.html#cost-function"><i class="fa fa-check"></i><b>2.5</b> Cost Function</a></li>
<li class="chapter" data-level="2.6" data-path="week-1.html"><a href="week-1.html#cost-function---intution-i"><i class="fa fa-check"></i><b>2.6</b> Cost Function - Intution I</a></li>
<li class="chapter" data-level="2.7" data-path="week-1.html"><a href="week-1.html#cost-function---intution-ii"><i class="fa fa-check"></i><b>2.7</b> Cost Function - Intution II</a></li>
<li class="chapter" data-level="2.8" data-path="week-1.html"><a href="week-1.html#gradient-descent"><i class="fa fa-check"></i><b>2.8</b> Gradient Descent</a></li>
<li class="chapter" data-level="2.9" data-path="week-1.html"><a href="week-1.html#gradient-descent-intution"><i class="fa fa-check"></i><b>2.9</b> Gradient Descent Intution</a></li>
<li class="chapter" data-level="2.10" data-path="week-1.html"><a href="week-1.html#gradient-descent-for-linear-regression"><i class="fa fa-check"></i><b>2.10</b> Gradient Descent for Linear Regression</a></li>
<li class="chapter" data-level="2.11" data-path="week-1.html"><a href="week-1.html#matrices-and-vectors"><i class="fa fa-check"></i><b>2.11</b> Matrices and Vectors</a></li>
<li class="chapter" data-level="2.12" data-path="week-1.html"><a href="week-1.html#addition-and-scalar-multiplication"><i class="fa fa-check"></i><b>2.12</b> Addition and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.13" data-path="week-1.html"><a href="week-1.html#matrix-vector-multiplication"><i class="fa fa-check"></i><b>2.13</b> Matrix-Vector Multiplication</a></li>
<li class="chapter" data-level="2.14" data-path="week-1.html"><a href="week-1.html#matrix-matrix-multiplication"><i class="fa fa-check"></i><b>2.14</b> Matrix-Matrix Multiplication</a></li>
<li class="chapter" data-level="2.15" data-path="week-1.html"><a href="week-1.html#matrix-multiplication-properties"><i class="fa fa-check"></i><b>2.15</b> Matrix Multiplication Properties</a></li>
<li class="chapter" data-level="2.16" data-path="week-1.html"><a href="week-1.html#inverse-and-transpose"><i class="fa fa-check"></i><b>2.16</b> Inverse and Transpose</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2.html"><a href="week-2.html"><i class="fa fa-check"></i><b>3</b> Week 2</a><ul>
<li class="chapter" data-level="3.1" data-path="week-2.html"><a href="week-2.html#multiple-features"><i class="fa fa-check"></i><b>3.1</b> Multiple features</a></li>
<li class="chapter" data-level="3.2" data-path="week-2.html"><a href="week-2.html#gradient-descent-for-multiple-variables"><i class="fa fa-check"></i><b>3.2</b> Gradient Descent for Multiple Variables</a></li>
<li class="chapter" data-level="3.3" data-path="week-2.html"><a href="week-2.html#gradient-descent-in-practice-i---feature-scaling"><i class="fa fa-check"></i><b>3.3</b> Gradient Descent in Practice I - Feature Scaling</a></li>
<li class="chapter" data-level="3.4" data-path="week-2.html"><a href="week-2.html#gradient-descent-in-practice-ii---learning-rate"><i class="fa fa-check"></i><b>3.4</b> Gradient Descent in Practice II - Learning Rate</a></li>
<li class="chapter" data-level="3.5" data-path="week-2.html"><a href="week-2.html#features-and-polynomial-regression"><i class="fa fa-check"></i><b>3.5</b> Features and Polynomial Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="week-2.html"><a href="week-2.html#polynomial-regression"><i class="fa fa-check"></i><b>3.5.1</b> Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="week-2.html"><a href="week-2.html#normal-equation"><i class="fa fa-check"></i><b>3.6</b> Normal Equation</a></li>
<li class="chapter" data-level="3.7" data-path="week-2.html"><a href="week-2.html#normal-equation-non-invertibility"><i class="fa fa-check"></i><b>3.7</b> Normal Equation Non-invertibility</a></li>
<li class="chapter" data-level="3.8" data-path="week-2.html"><a href="week-2.html#octave---basic-operations"><i class="fa fa-check"></i><b>3.8</b> Octave - Basic Operations</a></li>
<li class="chapter" data-level="3.9" data-path="week-2.html"><a href="week-2.html#moving-data-around"><i class="fa fa-check"></i><b>3.9</b> Moving Data Around</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-3.html"><a href="week-3.html"><i class="fa fa-check"></i><b>4</b> Week 3</a><ul>
<li class="chapter" data-level="4.1" data-path="week-3.html"><a href="week-3.html#classification"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="week-3.html"><a href="week-3.html#logistic-regression---hpothesis-representation"><i class="fa fa-check"></i><b>4.2</b> Logistic regression - Hpothesis Representation</a></li>
<li class="chapter" data-level="4.3" data-path="week-3.html"><a href="week-3.html#decision-boundary"><i class="fa fa-check"></i><b>4.3</b> Decision Boundary</a></li>
<li class="chapter" data-level="4.4" data-path="week-3.html"><a href="week-3.html#logistic-regression---cost-function"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression - Cost Function</a></li>
<li class="chapter" data-level="4.5" data-path="week-3.html"><a href="week-3.html#simplified-cost-functin-and-gradient-descent---logreg"><i class="fa fa-check"></i><b>4.5</b> Simplified Cost Functin and Gradient Descent - LogReg</a><ul>
<li class="chapter" data-level="4.5.1" data-path="week-1.html"><a href="week-1.html#gradient-descent"><i class="fa fa-check"></i><b>4.5.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.5.2" data-path="week-3.html"><a href="week-3.html#partial-derivative-of-jθ"><i class="fa fa-check"></i><b>4.5.2</b> Partial derivative of J(θ)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="week-3.html"><a href="week-3.html#advanced-optimization"><i class="fa fa-check"></i><b>4.6</b> Advanced Optimization</a></li>
<li class="chapter" data-level="4.7" data-path="week-3.html"><a href="week-3.html#multiclass-classification-one-vs-all"><i class="fa fa-check"></i><b>4.7</b> Multiclass Classification: One-vs-all</a></li>
<li class="chapter" data-level="4.8" data-path="week-3.html"><a href="week-3.html#the-problem-of-overfitting"><i class="fa fa-check"></i><b>4.8</b> The Problem of Overfitting</a></li>
<li class="chapter" data-level="4.9" data-path="week-3.html"><a href="week-3.html#regularization-cost-function"><i class="fa fa-check"></i><b>4.9</b> Regularization Cost Function</a></li>
<li class="chapter" data-level="4.10" data-path="week-3.html"><a href="week-3.html#regularized-linear-regression"><i class="fa fa-check"></i><b>4.10</b> Regularized Linear Regression</a><ul>
<li class="chapter" data-level="4.10.1" data-path="week-3.html"><a href="week-3.html#gradient-descent-1"><i class="fa fa-check"></i><b>4.10.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.10.2" data-path="week-2.html"><a href="week-2.html#normal-equation"><i class="fa fa-check"></i><b>4.10.2</b> Normal Equation</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="week-3.html"><a href="week-3.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>4.11</b> Regularized Logistic Regression</a><ul>
<li class="chapter" data-level="4.11.1" data-path="week-1.html"><a href="week-1.html#cost-function"><i class="fa fa-check"></i><b>4.11.1</b> Cost Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-4.html"><a href="week-4.html"><i class="fa fa-check"></i><b>5</b> Week 4</a><ul>
<li class="chapter" data-level="5.1" data-path="week-4.html"><a href="week-4.html#model-representation-i"><i class="fa fa-check"></i><b>5.1</b> Model Representation I</a></li>
<li class="chapter" data-level="5.2" data-path="week-4.html"><a href="week-4.html#model-representation-ii"><i class="fa fa-check"></i><b>5.2</b> Model Representation II</a><ul>
<li class="chapter" data-level="5.2.1" data-path="week-4.html"><a href="week-4.html#examples-and-intuitions-i"><i class="fa fa-check"></i><b>5.2.1</b> Examples and Intuitions I</a></li>
<li class="chapter" data-level="5.2.2" data-path="week-4.html"><a href="week-4.html#examples-and-intuitions-ii"><i class="fa fa-check"></i><b>5.2.2</b> Examples and Intuitions II</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="week-4.html"><a href="week-4.html#multiclass-classification"><i class="fa fa-check"></i><b>5.3</b> Multiclass Classification</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-5.html"><a href="week-5.html"><i class="fa fa-check"></i><b>6</b> Week 5</a><ul>
<li class="chapter" data-level="6.1" data-path="week-5.html"><a href="week-5.html#cost-function-and-backpropagation---neural-networks"><i class="fa fa-check"></i><b>6.1</b> Cost Function and Backpropagation - Neural Networks</a><ul>
<li class="chapter" data-level="6.1.1" data-path="week-1.html"><a href="week-1.html#cost-function"><i class="fa fa-check"></i><b>6.1.1</b> Cost Function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="week-5.html"><a href="week-5.html#backpropagation-algorithm"><i class="fa fa-check"></i><b>6.2</b> Backpropagation Algorithm</a><ul>
<li class="chapter" data-level="6.2.1" data-path="week-5.html"><a href="week-5.html#back-propagation-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> Back propagation Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="week-5.html"><a href="week-5.html#backpropagation-intuition"><i class="fa fa-check"></i><b>6.3</b> Backpropagation Intuition</a></li>
<li class="chapter" data-level="6.4" data-path="week-5.html"><a href="week-5.html#implementation-note-unrolling-parameters"><i class="fa fa-check"></i><b>6.4</b> Implementation Note: Unrolling Parameters</a></li>
<li class="chapter" data-level="6.5" data-path="week-5.html"><a href="week-5.html#gradient-checking"><i class="fa fa-check"></i><b>6.5</b> Gradient Checking</a></li>
<li class="chapter" data-level="6.6" data-path="week-5.html"><a href="week-5.html#random-initialization"><i class="fa fa-check"></i><b>6.6</b> Random Initialization</a></li>
<li class="chapter" data-level="6.7" data-path="week-5.html"><a href="week-5.html#putting-it-together"><i class="fa fa-check"></i><b>6.7</b> Putting it Together</a><ul>
<li class="chapter" data-level="6.7.1" data-path="week-5.html"><a href="week-5.html#training-a-neural-network"><i class="fa fa-check"></i><b>6.7.1</b> Training a Neural Network</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="7" data-path="charlie-chapter.html"><a href="charlie-chapter.html"><i class="fa fa-check"></i><b>7</b> Charlie Chapter</a><ul>
<li class="chapter" data-level="7.1" data-path="charlie-chapter.html"><a href="charlie-chapter.html#including-plots"><i class="fa fa-check"></i><b>7.1</b> Including Plots</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="what-happens-here.html"><a href="what-happens-here.html"><i class="fa fa-check"></i><b>8</b> What happens here?</a></li>
<li class="chapter" data-level="9" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html"><i class="fa fa-check"></i><b>9</b> 1 ML Ex 2 R implementationa: Logistic Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#introduction"><i class="fa fa-check"></i><b>9.1</b> 1.1 Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#data-prep"><i class="fa fa-check"></i><b>9.1.1</b> Data Prep</a></li>
<li class="chapter" data-level="9.1.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#visualizing-the-data"><i class="fa fa-check"></i><b>9.1.2</b> Visualizing the data</a></li>
<li class="chapter" data-level="9.1.3" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#warmup-exercise-sigmoid-function"><i class="fa fa-check"></i><b>9.1.3</b> Warmup exercise: sigmoid function</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cost-function-and-gradient"><i class="fa fa-check"></i><b>9.2</b> 1.2 Cost function and gradient</a></li>
<li class="chapter" data-level="9.3" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#learning-parameters-using-fminunc"><i class="fa fa-check"></i><b>9.3</b> 1.3 Learning parameters using fminunc</a></li>
<li class="chapter" data-level="9.4" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#plot-the-decision-boundary"><i class="fa fa-check"></i><b>9.4</b> 1.4 Plot the decision boundary</a></li>
<li class="chapter" data-level="9.5" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#evaluating-logistic-regression"><i class="fa fa-check"></i><b>9.5</b> 1.5 Evaluating logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-3.html"><a href="week-3.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>10</b> 2 Regularized logistic regression</a><ul>
<li class="chapter" data-level="10.1" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html"><i class="fa fa-check"></i><b>10.1</b> 2.1 Visualizing the data</a></li>
<li class="chapter" data-level="10.2" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#feature-mapping"><i class="fa fa-check"></i><b>10.2</b> 2.2 Feature mapping</a></li>
<li class="chapter" data-level="10.3" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#cost-function-and-gradient-1"><i class="fa fa-check"></i><b>10.3</b> 2.3 Cost function and gradient</a></li>
<li class="chapter" data-level="10.4" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#learning-parameters-using-fminunc-1"><i class="fa fa-check"></i><b>10.4</b> 2.4 Learning parameters using fminunc</a></li>
<li class="chapter" data-level="10.5" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#plotting-the-decision-boundary"><i class="fa fa-check"></i><b>10.5</b> 2.5 Plotting the decision boundary</a></li>
<li class="chapter" data-level="10.6" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#varying-lambda-levels"><i class="fa fa-check"></i><b>10.6</b> 2.4 Varying lambda levels</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html"><i class="fa fa-check"></i><b>11</b> ML - Programming Exercise 3</a><ul>
<li class="chapter" data-level="11.1" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html#introduction-multi-class-classification"><i class="fa fa-check"></i><b>11.1</b> Introduction: Multi-class Classification</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html#dataset"><i class="fa fa-check"></i><b>11.1.1</b> 1.1 Dataset</a></li>
<li class="chapter" data-level="11.1.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#visualizing-the-data"><i class="fa fa-check"></i><b>11.1.2</b> 1.2 Visualizing the data</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Notes - Andrew Ng</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-5" class="section level1">
<h1><span class="header-section-number">6</span> Week 5</h1>
<div id="cost-function-and-backpropagation---neural-networks" class="section level2">
<h2><span class="header-section-number">6.1</span> Cost Function and Backpropagation - Neural Networks</h2>
<div id="cost-function" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Cost Function</h3>
<p>Let’s first define a few variables that we will need to use:</p>
<ul>
<li>L = total number of layers in the network - below L = 4</li>
<li><span class="math inline">\(s_l\)</span> = number of units (not counting bias unit) in layer l
<ul>
<li><span class="math inline">\(s_1 = 3\)</span>, <span class="math inline">\(s_2 = 5\)</span>, <span class="math inline">\(s_3 = 5\)</span>, <span class="math inline">\(s_4 = s_l = K = 4\)</span></li>
</ul></li>
<li>K = number of output units/classes - below K = 4
<ul>
<li>in binary classification K is 1.</li>
<li>in multiclassification problems K &gt;= 3 also <span class="math inline">\(s_l = K\)</span></li>
</ul></li>
</ul>
<p><img src="Neural%20Network%20cost%20fun.png" /></p>
<p>Recall that in neural networks, we may have many output nodes. We denote <span class="math inline">\(h_\Theta(x)_k\)</span> as being a hypothesis that results in the <span class="math inline">\(k^{th}\)</span> output. Our cost function for neural networks is going to be a generalization of the one we used for logistic regression. Recall that the cost function for regularized logistic regression was:</p>
<p><span class="math inline">\(J(\theta) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2\)</span></p>
<p>Vectorized in r as</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="week-5.html#cb58-1"></a>        cost &lt;-<span class="st">  </span><span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>((<span class="kw">t</span>(<span class="op">-</span>y)<span class="op">%*%</span><span class="kw">log</span>(<span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta))<span class="op">-</span></span>
<span id="cb58-2"><a href="week-5.html#cb58-2"></a><span class="st">                        </span><span class="kw">t</span>((<span class="dv">1</span><span class="op">-</span>y))<span class="op">%*%</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span><span class="st"> </span><span class="kw">sigmoid</span>(X<span class="op">%*%</span>theta)))) <span class="op">+</span><span class="st"> </span></span>
<span id="cb58-3"><a href="week-5.html#cb58-3"></a><span class="st">          </span>( (lambda<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span><span class="st"> </span>(<span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(theta)) <span class="op">%*%</span><span class="st"> </span>(<span class="kw">c</span>(<span class="dv">0</span>, theta[<span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(theta)]))<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb58-4"><a href="week-5.html#cb58-4"></a></span>
<span id="cb58-5"><a href="week-5.html#cb58-5"></a>        grad &lt;-<span class="st">  </span><span class="dv">1</span><span class="op">/</span>m <span class="op">*</span><span class="st"> </span>(<span class="kw">t</span>(<span class="kw">sigmoid</span>(X <span class="op">%*%</span><span class="st"> </span>theta) <span class="op">-</span><span class="st"> </span>y)<span class="op">%*%</span>X) <span class="op">+</span><span class="st"> </span>((lambda<span class="op">/</span>m)<span class="op">*</span>(<span class="kw">c</span>(<span class="dv">0</span>, theta[<span class="dv">2</span><span class="op">:</span><span class="kw">length</span>(theta)])))</span></code></pre></div>
<p>For neural networks, it is going to be slightly more complicated:</p>
<p><span class="math display">\[\begin{gather*} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}\]</span></p>
<p>We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.</p>
<p>In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.</p>
<p>Note:</p>
<ul>
<li>the double sum simply adds up the logistic regression costs calculated for each cell in the output layer</li>
<li>the triple sum simply adds up the squares of all the individual Θs in the entire network.</li>
<li>the i in the triple sum does <strong>not</strong> refer to training example i</li>
</ul>
</div>
</div>
<div id="backpropagation-algorithm" class="section level2">
<h2><span class="header-section-number">6.2</span> Backpropagation Algorithm</h2>
<p>“Backpropagation” is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression. Our goal is to compute:</p>
<p><span class="math inline">\(\min_\Theta J(\Theta)\)</span></p>
<p>That is, we want to minimize our cost function J using an optimal set of parameters in theta. In this section we’ll look at the equations we use to compute the partial derivative of J(Θ):</p>
<p><span class="math inline">\(\dfrac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}}\)</span></p>
<p>To do so, we use the following algorithm:</p>
<p><img src="backpropagation.png" /></p>
<div id="back-propagation-algorithm" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Back propagation Algorithm</h3>
<p>Given training set <span class="math inline">\(\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace\)</span></p>
<ul>
<li>Set <span class="math inline">\(\Delta^{(l)}_{i,j} := 0\)</span> for all (l,i,j), (hence you end up having a matrix full of zeros)</li>
</ul>
<p>For training example t =1 to m:</p>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(a^{(1)} := x^{(t)}\)</span></li>
<li>Perform forward propagation to compute <span class="math inline">\(a^{(l)}\)</span> for l=2,3,…,L</li>
</ol>
<p><img src="backpropagation2.png" /></p>
<ol start="3" style="list-style-type: decimal">
<li>Using <span class="math inline">\(y^{(t)}\)</span>, compute <span class="math inline">\(\delta^{(L)} = a^{(L)} - y^{(t)}\)</span></li>
</ol>
<p>Where L is our total number of layers and <span class="math inline">\(a^{(L)}\)</span> is the vector of outputs of the activation units for the last layer. So our “error values” for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:</p>
<ol start="4" style="list-style-type: decimal">
<li>Compute <span class="math inline">\(\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}\)</span> using <span class="math inline">\(\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\)</span></li>
</ol>
<p>The delta values of layer l are calculated by multiplying the delta values in the next layer with the theta matrix of layer l. We then element-wise multiply that with a function called g’, or g-prime, which is the derivative of the activation function g evaluated with the input values given by <span class="math inline">\(z^{(l)}\)</span>.</p>
<p>The g-prime derivative terms can also be written out as:</p>
<p><span class="math inline">\(g&#39;(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})\)</span></p>
<ol start="5" style="list-style-type: decimal">
<li><span class="math inline">\(\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}\)</span> or with vectorization, <span class="math inline">\(\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T\)</span></li>
</ol>
<p>Hence we update our new <span class="math inline">\(\Delta\)</span> matrix.</p>
<ul>
<li><span class="math inline">\(D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)\)</span>, if j≠0.</li>
<li><span class="math inline">\(D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}\)</span> if j=0</li>
</ul>
<p>The capital-delta matrix D is used as an “accumulator” to add up our values as we go along and eventually compute our partial derivative. Thus we get <span class="math inline">\(\frac {\partial J(\Theta)} {\partial \Theta_{ij}^{(l)}} = D_{ij}^{(l)}\)</span>.</p>
</div>
</div>
<div id="backpropagation-intuition" class="section level2">
<h2><span class="header-section-number">6.3</span> Backpropagation Intuition</h2>
<p>Recall that the cost function for a neural network is:</p>
<p><span class="math display">\[\begin{gather*}J(\Theta) = - \frac{1}{m} \sum_{t=1}^m\sum_{k=1}^K \left[ y^{(t)}_k \ \log (h_\Theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\Theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \Theta_{j,i}^{(l)})^2\end{gather*}\]</span></p>
<p>If we consider simple non-multiclass classification (k = 1) and disregard regularization, the cost is computed with:</p>
<p><span class="math inline">\(cost(t) =y^{(t)} \ \log (h_\Theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\Theta(x^{(t)}))\)</span></p>
<p>Intuitively, <span class="math inline">\(\delta_j^{(l)}\)</span> is the “error” for <span class="math inline">\(a^{(l)}_j\)</span> (unit j in layer l). More formally, the delta values are actually the derivative of the cost function:</p>
<p><span class="math inline">\(\delta_j^{(l)} = \dfrac{\partial cost(t)}{\partial z_j^{(l)}}\)</span></p>
<p>Recall that our derivative is the slope of a line tangent to the cost function, so the steeper the slope the more incorrect we are. Let us consider the following neural network below and see how we could calculate some <span class="math inline">\(\delta_j^{(l)}\)</span>:</p>
<p><img src="backpropagation_intution.png" /></p>
<p>In the image above, to calculate <span class="math inline">\(\delta_2^{(2)}\)</span>, we multiply the weights <span class="math inline">\(\Theta_{12}^{(2)}\)</span> and <span class="math inline">\(\Theta_{22}^{(2)}\)</span> by their respective <span class="math inline">\(\delta\)</span> values found to the right of each edge. So we get <span class="math inline">\(\delta_2^{(2)} = \Theta_{12}^{(2)} *\delta_1^{(3)} + \Theta_{22}^{(2)} *\delta_2^{(3)}\)</span>. To calculate every single possible <span class="math inline">\(\delta_j^{(l)}\)</span>, we could start from the right of our diagram. We can think of our edges as our <span class="math inline">\(\Theta_{ij}\)</span>. Going from right to left, to calculate the value of <span class="math inline">\(\delta_j^{(l)}\)</span>, you can just take the over all sum of each weight times the <span class="math inline">\(\delta\)</span> it is coming from. Hence, another example would be <span class="math inline">\(\delta_2^{(3)} =\Theta_{12}^{(3)} *\delta_1^{(4)}\)</span>.</p>
</div>
<div id="implementation-note-unrolling-parameters" class="section level2">
<h2><span class="header-section-number">6.4</span> Implementation Note: Unrolling Parameters</h2>
<p>With neural networks, we are working with sets of matrices:</p>
<p><span class="math display">\[\begin{align} \Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}, \dots \newline D^{(1)}, D^{(2)}, D^{(3)}, \dots \end{align}\]</span></p>
<p>In order to use optimizing functions such as “fminunc()”, we will want to “unroll” all the elements and put them into one long vector:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="week-5.html#cb59-1"></a>thetaVector =<span class="st"> </span>[ <span class="kw">Theta1</span>(<span class="op">:</span>); <span class="kw">Theta2</span>(<span class="op">:</span>); <span class="kw">Theta3</span>(<span class="op">:</span>); ]</span>
<span id="cb59-2"><a href="week-5.html#cb59-2"></a>deltaVector =<span class="st"> </span>[ <span class="kw">D1</span>(<span class="op">:</span>); <span class="kw">D2</span>(<span class="op">:</span>); <span class="kw">D3</span>(<span class="op">:</span>) ]</span></code></pre></div>
<p>If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11, then we can get back our original matrices from the “unrolled” versions as follows:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="week-5.html#cb60-1"></a>Theta1 =<span class="st"> </span><span class="kw">reshape</span>(<span class="kw">thetaVector</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">110</span>),<span class="dv">10</span>,<span class="dv">11</span>)</span>
<span id="cb60-2"><a href="week-5.html#cb60-2"></a>Theta2 =<span class="st"> </span><span class="kw">reshape</span>(<span class="kw">thetaVector</span>(<span class="dv">111</span><span class="op">:</span><span class="dv">220</span>),<span class="dv">10</span>,<span class="dv">11</span>)</span>
<span id="cb60-3"><a href="week-5.html#cb60-3"></a>Theta3 =<span class="st"> </span><span class="kw">reshape</span>(<span class="kw">thetaVector</span>(<span class="dv">221</span><span class="op">:</span><span class="dv">231</span>),<span class="dv">1</span>,<span class="dv">11</span>)</span></code></pre></div>
<p>To summarize:</p>
<p><img src="unrollingParameters.png" /></p>
</div>
<div id="gradient-checking" class="section level2">
<h2><span class="header-section-number">6.5</span> Gradient Checking</h2>
<p>Gradient checking will assure that our backpropagation works as intended. We can approximate the derivative of our cost function with:</p>
<p><span class="math inline">\(\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}\)</span></p>
<p>With multiple theta matrices, we can approximate the derivative <strong>with respect to</strong> <span class="math inline">\(Θ_j\)</span> as follows:</p>
<p><span class="math inline">\(\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}\)</span></p>
<p>A small value for <span class="math inline">\({\epsilon}\)</span> (epsilon) such as <span class="math inline">\({\epsilon = 10^{-4}}\)</span>, guarantees that the math works out properly. If the value for <span class="math inline">\(\epsilon\)</span> is too small, we can end up with numerical problems.</p>
<p>Hence, we are only adding or subtracting epsilon to the <span class="math inline">\(\Theta_j\)</span> matrix. In octave we can do it as follows:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="week-5.html#cb61-1"></a>epsilon =<span class="st"> </span><span class="fl">1e-4</span>;</span>
<span id="cb61-2"><a href="week-5.html#cb61-2"></a><span class="cf">for</span> i =<span class="st"> </span><span class="dv">1</span><span class="op">:</span>n,</span>
<span id="cb61-3"><a href="week-5.html#cb61-3"></a>  thetaPlus =<span class="st"> </span>theta;</span>
<span id="cb61-4"><a href="week-5.html#cb61-4"></a>  <span class="kw">thetaPlus</span>(i) <span class="op">+</span><span class="er">=</span><span class="st"> </span>epsilon;</span>
<span id="cb61-5"><a href="week-5.html#cb61-5"></a>  thetaMinus =<span class="st"> </span>theta;</span>
<span id="cb61-6"><a href="week-5.html#cb61-6"></a>  <span class="kw">thetaMinus</span>(i) <span class="op">-</span><span class="er">=</span><span class="st"> </span>epsilon;</span>
<span id="cb61-7"><a href="week-5.html#cb61-7"></a>  <span class="kw">gradApprox</span>(i) =<span class="st"> </span>(<span class="kw">J</span>(thetaPlus) <span class="op">-</span><span class="st"> </span><span class="kw">J</span>(thetaMinus))<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>epsilon)</span>
<span id="cb61-8"><a href="week-5.html#cb61-8"></a>end;</span></code></pre></div>
<p>We previously saw how to calculate the deltaVector. So once we compute our gradApprox vector, we can check that gradApprox ≈ deltaVector.</p>
<p>Once you have verified <strong>once</strong> that your backpropagation algorithm is correct, you don’t need to compute gradApprox again. The code to compute gradApprox can be very slow.</p>
</div>
<div id="random-initialization" class="section level2">
<h2><span class="header-section-number">6.6</span> Random Initialization</h2>
<p>Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly. Instead we can randomly initialize our weights for our <span class="math inline">\(\Theta\)</span> matrices using the following method:</p>
<p><img src="symmetryBreaking.png" />
Hence, we initialize each <span class="math inline">\(\Theta^{(l)}_{ij}\)</span> to a random value between <span class="math inline">\([-\epsilon,\epsilon]\)</span>. Using the above formula guarantees that we get the desired bound. The same procedure applies to all the <span class="math inline">\(\Theta\)</span>s. Below is some working code you could use to experiment.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="week-5.html#cb62-1"></a><span class="co"># If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.</span></span>
<span id="cb62-2"><a href="week-5.html#cb62-2"></a></span>
<span id="cb62-3"><a href="week-5.html#cb62-3"></a>Theta1 =<span class="st"> </span><span class="kw">rand</span>(<span class="dv">10</span>,<span class="dv">11</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>INIT_EPSILON) <span class="op">-</span><span class="st"> </span>INIT_EPSILON;</span>
<span id="cb62-4"><a href="week-5.html#cb62-4"></a>Theta2 =<span class="st"> </span><span class="kw">rand</span>(<span class="dv">10</span>,<span class="dv">11</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>INIT_EPSILON) <span class="op">-</span><span class="st"> </span>INIT_EPSILON;</span>
<span id="cb62-5"><a href="week-5.html#cb62-5"></a>Theta3 =<span class="st"> </span><span class="kw">rand</span>(<span class="dv">1</span>,<span class="dv">11</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>INIT_EPSILON) <span class="op">-</span><span class="st"> </span>INIT_EPSILON;</span></code></pre></div>
<p>rand(x,y) is just a function in octave that will initialize a matrix of random real numbers between 0 and 1.</p>
<p>(Note: the epsilon used above is unrelated to the epsilon from Gradient Checking)</p>
</div>
<div id="putting-it-together" class="section level2">
<h2><span class="header-section-number">6.7</span> Putting it Together</h2>
<p>First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have.</p>
<ul>
<li>Number of input units = dimension of features <span class="math inline">\(x^{(i)}\)</span></li>
<li>Number of output units = number of classes</li>
<li>Number of hidden units per layer = usually the more the better (must balance with cost of computation as it increases with more hidden units)</li>
<li>Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.</li>
</ul>
<div id="training-a-neural-network" class="section level3">
<h3><span class="header-section-number">6.7.1</span> Training a Neural Network</h3>
<ol style="list-style-type: decimal">
<li>Randomly initialize the weights</li>
<li>Implement forward propagation to get <span class="math inline">\(h_\Theta(x^{(i)})\)</span> for any <span class="math inline">\(x^{(i)}\)</span></li>
<li>Implement the cost function</li>
<li>Implement backpropagation to compute partial derivatives</li>
<li>Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.</li>
<li>Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.</li>
</ol>
<p>When we perform forward and back propagation, we loop on every training example:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="week-5.html#cb63-1"></a>or i =<span class="st"> </span><span class="dv">1</span><span class="op">:</span>m,</span>
<span id="cb63-2"><a href="week-5.html#cb63-2"></a>   Perform forward propagation and backpropagation using <span class="kw">example</span> (<span class="kw">x</span>(i),<span class="kw">y</span>(i))</span>
<span id="cb63-3"><a href="week-5.html#cb63-3"></a>   (Get activations <span class="kw">a</span>(l) and delta terms <span class="kw">d</span>(l) <span class="cf">for</span> <span class="dt">l =</span> <span class="dv">2</span>,...,L</span></code></pre></div>
<p>The following image gives us an intuition of what is happening as we are implementing our neural network:</p>
<p><img src="puttingItTogether.png" /></p>
<p>Ideally, you want <span class="math inline">\(h_\Theta(x^{(i)})\approx y^{(i)}\)</span>. This will minimize our cost function. However, keep in mind that <span class="math inline">\(J(\Theta)\)</span> is not convex and thus we can end up in a local minimum instead.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdw.pdf", "bookdw.epub"]
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

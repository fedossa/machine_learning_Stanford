<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Week 4 | Machine Learning Notes - Andrew Ng</title>
  <meta name="description" content="Study material for ML - Stanford University" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Week 4 | Machine Learning Notes - Andrew Ng" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Study material for ML - Stanford University" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Week 4 | Machine Learning Notes - Andrew Ng" />
  
  <meta name="twitter:description" content="Study material for ML - Stanford University" />
  

<meta name="author" content="Fikir Worku Edossa" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-3.html"/>
<link rel="next" href="week-5.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="week-1.html"><a href="week-1.html"><i class="fa fa-check"></i><b>2</b> Week 1</a><ul>
<li class="chapter" data-level="2.1" data-path="week-1.html"><a href="week-1.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="2.2" data-path="week-1.html"><a href="week-1.html#supervised-learning"><i class="fa fa-check"></i><b>2.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="2.2.1" data-path="week-1.html"><a href="week-1.html#supervised-learning-note"><i class="fa fa-check"></i><b>2.2.1</b> Supervised Learning (note)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="week-1.html"><a href="week-1.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.3</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="2.4" data-path="week-1.html"><a href="week-1.html#model-representation"><i class="fa fa-check"></i><b>2.4</b> Model Representation</a></li>
<li class="chapter" data-level="2.5" data-path="week-1.html"><a href="week-1.html#cost-function"><i class="fa fa-check"></i><b>2.5</b> Cost Function</a></li>
<li class="chapter" data-level="2.6" data-path="week-1.html"><a href="week-1.html#cost-function---intution-i"><i class="fa fa-check"></i><b>2.6</b> Cost Function - Intution I</a></li>
<li class="chapter" data-level="2.7" data-path="week-1.html"><a href="week-1.html#cost-function---intution-ii"><i class="fa fa-check"></i><b>2.7</b> Cost Function - Intution II</a></li>
<li class="chapter" data-level="2.8" data-path="week-1.html"><a href="week-1.html#gradient-descent"><i class="fa fa-check"></i><b>2.8</b> Gradient Descent</a></li>
<li class="chapter" data-level="2.9" data-path="week-1.html"><a href="week-1.html#gradient-descent-intution"><i class="fa fa-check"></i><b>2.9</b> Gradient Descent Intution</a></li>
<li class="chapter" data-level="2.10" data-path="week-1.html"><a href="week-1.html#gradient-descent-for-linear-regression"><i class="fa fa-check"></i><b>2.10</b> Gradient Descent for Linear Regression</a></li>
<li class="chapter" data-level="2.11" data-path="week-1.html"><a href="week-1.html#matrices-and-vectors"><i class="fa fa-check"></i><b>2.11</b> Matrices and Vectors</a></li>
<li class="chapter" data-level="2.12" data-path="week-1.html"><a href="week-1.html#addition-and-scalar-multiplication"><i class="fa fa-check"></i><b>2.12</b> Addition and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.13" data-path="week-1.html"><a href="week-1.html#matrix-vector-multiplication"><i class="fa fa-check"></i><b>2.13</b> Matrix-Vector Multiplication</a></li>
<li class="chapter" data-level="2.14" data-path="week-1.html"><a href="week-1.html#matrix-matrix-multiplication"><i class="fa fa-check"></i><b>2.14</b> Matrix-Matrix Multiplication</a></li>
<li class="chapter" data-level="2.15" data-path="week-1.html"><a href="week-1.html#matrix-multiplication-properties"><i class="fa fa-check"></i><b>2.15</b> Matrix Multiplication Properties</a></li>
<li class="chapter" data-level="2.16" data-path="week-1.html"><a href="week-1.html#inverse-and-transpose"><i class="fa fa-check"></i><b>2.16</b> Inverse and Transpose</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2.html"><a href="week-2.html"><i class="fa fa-check"></i><b>3</b> Week 2</a><ul>
<li class="chapter" data-level="3.1" data-path="week-2.html"><a href="week-2.html#multiple-features"><i class="fa fa-check"></i><b>3.1</b> Multiple features</a></li>
<li class="chapter" data-level="3.2" data-path="week-2.html"><a href="week-2.html#gradient-descent-for-multiple-variables"><i class="fa fa-check"></i><b>3.2</b> Gradient Descent for Multiple Variables</a></li>
<li class="chapter" data-level="3.3" data-path="week-2.html"><a href="week-2.html#gradient-descent-in-practice-i---feature-scaling"><i class="fa fa-check"></i><b>3.3</b> Gradient Descent in Practice I - Feature Scaling</a></li>
<li class="chapter" data-level="3.4" data-path="week-2.html"><a href="week-2.html#gradient-descent-in-practice-ii---learning-rate"><i class="fa fa-check"></i><b>3.4</b> Gradient Descent in Practice II - Learning Rate</a></li>
<li class="chapter" data-level="3.5" data-path="week-2.html"><a href="week-2.html#features-and-polynomial-regression"><i class="fa fa-check"></i><b>3.5</b> Features and Polynomial Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="week-2.html"><a href="week-2.html#polynomial-regression"><i class="fa fa-check"></i><b>3.5.1</b> Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="week-2.html"><a href="week-2.html#normal-equation"><i class="fa fa-check"></i><b>3.6</b> Normal Equation</a></li>
<li class="chapter" data-level="3.7" data-path="week-2.html"><a href="week-2.html#normal-equation-non-invertibility"><i class="fa fa-check"></i><b>3.7</b> Normal Equation Non-invertibility</a></li>
<li class="chapter" data-level="3.8" data-path="week-2.html"><a href="week-2.html#octave---basic-operations"><i class="fa fa-check"></i><b>3.8</b> Octave - Basic Operations</a></li>
<li class="chapter" data-level="3.9" data-path="week-2.html"><a href="week-2.html#moving-data-around"><i class="fa fa-check"></i><b>3.9</b> Moving Data Around</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-3.html"><a href="week-3.html"><i class="fa fa-check"></i><b>4</b> Week 3</a><ul>
<li class="chapter" data-level="4.1" data-path="week-3.html"><a href="week-3.html#classification"><i class="fa fa-check"></i><b>4.1</b> Classification</a></li>
<li class="chapter" data-level="4.2" data-path="week-3.html"><a href="week-3.html#logistic-regression---hpothesis-representation"><i class="fa fa-check"></i><b>4.2</b> Logistic regression - Hpothesis Representation</a></li>
<li class="chapter" data-level="4.3" data-path="week-3.html"><a href="week-3.html#decision-boundary"><i class="fa fa-check"></i><b>4.3</b> Decision Boundary</a></li>
<li class="chapter" data-level="4.4" data-path="week-3.html"><a href="week-3.html#logistic-regression---cost-function"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression - Cost Function</a></li>
<li class="chapter" data-level="4.5" data-path="week-3.html"><a href="week-3.html#simplified-cost-functin-and-gradient-descent---logreg"><i class="fa fa-check"></i><b>4.5</b> Simplified Cost Functin and Gradient Descent - LogReg</a><ul>
<li class="chapter" data-level="4.5.1" data-path="week-1.html"><a href="week-1.html#gradient-descent"><i class="fa fa-check"></i><b>4.5.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.5.2" data-path="week-3.html"><a href="week-3.html#partial-derivative-of-jθ"><i class="fa fa-check"></i><b>4.5.2</b> Partial derivative of J(θ)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="week-3.html"><a href="week-3.html#advanced-optimization"><i class="fa fa-check"></i><b>4.6</b> Advanced Optimization</a></li>
<li class="chapter" data-level="4.7" data-path="week-3.html"><a href="week-3.html#multiclass-classification-one-vs-all"><i class="fa fa-check"></i><b>4.7</b> Multiclass Classification: One-vs-all</a></li>
<li class="chapter" data-level="4.8" data-path="week-3.html"><a href="week-3.html#the-problem-of-overfitting"><i class="fa fa-check"></i><b>4.8</b> The Problem of Overfitting</a></li>
<li class="chapter" data-level="4.9" data-path="week-3.html"><a href="week-3.html#regularization-cost-function"><i class="fa fa-check"></i><b>4.9</b> Regularization Cost Function</a></li>
<li class="chapter" data-level="4.10" data-path="week-3.html"><a href="week-3.html#regularized-linear-regression"><i class="fa fa-check"></i><b>4.10</b> Regularized Linear Regression</a><ul>
<li class="chapter" data-level="4.10.1" data-path="week-3.html"><a href="week-3.html#gradient-descent-1"><i class="fa fa-check"></i><b>4.10.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.10.2" data-path="week-2.html"><a href="week-2.html#normal-equation"><i class="fa fa-check"></i><b>4.10.2</b> Normal Equation</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="week-3.html"><a href="week-3.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>4.11</b> Regularized Logistic Regression</a><ul>
<li class="chapter" data-level="4.11.1" data-path="week-1.html"><a href="week-1.html#cost-function"><i class="fa fa-check"></i><b>4.11.1</b> Cost Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-4.html"><a href="week-4.html"><i class="fa fa-check"></i><b>5</b> Week 4</a><ul>
<li class="chapter" data-level="5.1" data-path="week-4.html"><a href="week-4.html#model-representation-i"><i class="fa fa-check"></i><b>5.1</b> Model Representation I</a></li>
<li class="chapter" data-level="5.2" data-path="week-4.html"><a href="week-4.html#model-representation-ii"><i class="fa fa-check"></i><b>5.2</b> Model Representation II</a><ul>
<li class="chapter" data-level="5.2.1" data-path="week-4.html"><a href="week-4.html#examples-and-intuitions-i"><i class="fa fa-check"></i><b>5.2.1</b> Examples and Intuitions I</a></li>
<li class="chapter" data-level="5.2.2" data-path="week-4.html"><a href="week-4.html#examples-and-intuitions-ii"><i class="fa fa-check"></i><b>5.2.2</b> Examples and Intuitions II</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="week-4.html"><a href="week-4.html#multiclass-classification"><i class="fa fa-check"></i><b>5.3</b> Multiclass Classification</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-5.html"><a href="week-5.html"><i class="fa fa-check"></i><b>6</b> Week 5</a><ul>
<li class="chapter" data-level="6.1" data-path="week-5.html"><a href="week-5.html#cost-function-and-backpropagation---neural-networks"><i class="fa fa-check"></i><b>6.1</b> Cost Function and Backpropagation - Neural Networks</a><ul>
<li class="chapter" data-level="6.1.1" data-path="week-1.html"><a href="week-1.html#cost-function"><i class="fa fa-check"></i><b>6.1.1</b> Cost Function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="week-5.html"><a href="week-5.html#backpropagation-algorithm"><i class="fa fa-check"></i><b>6.2</b> Backpropagation Algorithm</a><ul>
<li class="chapter" data-level="6.2.1" data-path="week-5.html"><a href="week-5.html#back-propagation-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> Back propagation Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="week-5.html"><a href="week-5.html#backpropagation-intuition"><i class="fa fa-check"></i><b>6.3</b> Backpropagation Intuition</a></li>
<li class="chapter" data-level="6.4" data-path="week-5.html"><a href="week-5.html#implementation-note-unrolling-parameters"><i class="fa fa-check"></i><b>6.4</b> Implementation Note: Unrolling Parameters</a></li>
<li class="chapter" data-level="6.5" data-path="week-5.html"><a href="week-5.html#gradient-checking"><i class="fa fa-check"></i><b>6.5</b> Gradient Checking</a></li>
<li class="chapter" data-level="6.6" data-path="week-5.html"><a href="week-5.html#random-initialization"><i class="fa fa-check"></i><b>6.6</b> Random Initialization</a></li>
<li class="chapter" data-level="6.7" data-path="week-5.html"><a href="week-5.html#putting-it-together"><i class="fa fa-check"></i><b>6.7</b> Putting it Together</a><ul>
<li class="chapter" data-level="6.7.1" data-path="week-5.html"><a href="week-5.html#training-a-neural-network"><i class="fa fa-check"></i><b>6.7.1</b> Training a Neural Network</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="7" data-path="charlie-chapter.html"><a href="charlie-chapter.html"><i class="fa fa-check"></i><b>7</b> Charlie Chapter</a><ul>
<li class="chapter" data-level="7.1" data-path="charlie-chapter.html"><a href="charlie-chapter.html#including-plots"><i class="fa fa-check"></i><b>7.1</b> Including Plots</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="what-happens-here.html"><a href="what-happens-here.html"><i class="fa fa-check"></i><b>8</b> What happens here?</a></li>
<li class="chapter" data-level="9" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html"><i class="fa fa-check"></i><b>9</b> 1 ML Ex 2 R implementationa: Logistic Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#introduction"><i class="fa fa-check"></i><b>9.1</b> 1.1 Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#data-prep"><i class="fa fa-check"></i><b>9.1.1</b> Data Prep</a></li>
<li class="chapter" data-level="9.1.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#visualizing-the-data"><i class="fa fa-check"></i><b>9.1.2</b> Visualizing the data</a></li>
<li class="chapter" data-level="9.1.3" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#warmup-exercise-sigmoid-function"><i class="fa fa-check"></i><b>9.1.3</b> Warmup exercise: sigmoid function</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#cost-function-and-gradient"><i class="fa fa-check"></i><b>9.2</b> 1.2 Cost function and gradient</a></li>
<li class="chapter" data-level="9.3" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#learning-parameters-using-fminunc"><i class="fa fa-check"></i><b>9.3</b> 1.3 Learning parameters using fminunc</a></li>
<li class="chapter" data-level="9.4" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#plot-the-decision-boundary"><i class="fa fa-check"></i><b>9.4</b> 1.4 Plot the decision boundary</a></li>
<li class="chapter" data-level="9.5" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#evaluating-logistic-regression"><i class="fa fa-check"></i><b>9.5</b> 1.5 Evaluating logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-3.html"><a href="week-3.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>10</b> 2 Regularized logistic regression</a><ul>
<li class="chapter" data-level="10.1" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html"><i class="fa fa-check"></i><b>10.1</b> 2.1 Visualizing the data</a></li>
<li class="chapter" data-level="10.2" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#feature-mapping"><i class="fa fa-check"></i><b>10.2</b> 2.2 Feature mapping</a></li>
<li class="chapter" data-level="10.3" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#cost-function-and-gradient-1"><i class="fa fa-check"></i><b>10.3</b> 2.3 Cost function and gradient</a></li>
<li class="chapter" data-level="10.4" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#learning-parameters-using-fminunc-1"><i class="fa fa-check"></i><b>10.4</b> 2.4 Learning parameters using fminunc</a></li>
<li class="chapter" data-level="10.5" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#plotting-the-decision-boundary"><i class="fa fa-check"></i><b>10.5</b> 2.5 Plotting the decision boundary</a></li>
<li class="chapter" data-level="10.6" data-path="regularized-logistic-regression.html"><a href="regularized-logistic-regression.html#varying-lambda-levels"><i class="fa fa-check"></i><b>10.6</b> 2.4 Varying lambda levels</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html"><i class="fa fa-check"></i><b>11</b> ML - Programming Exercise 3</a><ul>
<li class="chapter" data-level="11.1" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html#introduction-multi-class-classification"><i class="fa fa-check"></i><b>11.1</b> Introduction: Multi-class Classification</a><ul>
<li class="chapter" data-level="11.1.1" data-path="ml-programming-exercise-3.html"><a href="ml-programming-exercise-3.html#dataset"><i class="fa fa-check"></i><b>11.1.1</b> 1.1 Dataset</a></li>
<li class="chapter" data-level="11.1.2" data-path="ml-ex-2-r-implementationa-logistic-regression.html"><a href="ml-ex-2-r-implementationa-logistic-regression.html#visualizing-the-data"><i class="fa fa-check"></i><b>11.1.2</b> 1.2 Visualizing the data</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Notes - Andrew Ng</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-4" class="section level1">
<h1><span class="header-section-number">5</span> Week 4</h1>
<div id="model-representation-i" class="section level2">
<h2><span class="header-section-number">5.1</span> Model Representation I</h2>
<p>Let’s examine how we will represent a hypothesis function using neural networks. At a very simple level, neurons are basically computational units that take inputs (<strong>dendrites</strong>) as electrical inputs (called “spikes”) that are channeled to outputs (<strong>axons</strong>). In our model, our dendrites are like the input features <span class="math inline">\(x_1\cdots x_n\)</span>, and the output is the result of our hypothesis function. In this model our <span class="math inline">\(x_0\)</span> input node is sometimes called the “bias unit.” It is always equal to 1. In neural networks, we use the same logistic function as in classification, <span class="math inline">\(\frac{1}{1 + e^{-\theta^Tx}}\)</span>, yet we sometimes call it a sigmoid (logistic) <strong>activation</strong> function. In this situation, our “theta” parameters are sometimes called “weights”.</p>
<p>Visually, a simplistic representation looks like:</p>
<p><span class="math inline">\(\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline \end{bmatrix}\rightarrow\begin{bmatrix}\ \ \ \newline \end{bmatrix}\rightarrow h_\theta(x)\)</span></p>
<p>Our input nodes (layer 1), also known as the “input layer”, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer”.</p>
<p>We can have intermediate layers of nodes between the input and output layers called the “hidden layers.”</p>
<p>In this example, we label these intermediate or “hidden” layer nodes <span class="math inline">\(a^2_0 \cdots a^2_n\)</span> and call them “activation units.”</p>
<p><span class="math display">\[\begin{align}&amp; a_i^{(j)} = \text{&quot;activation&quot; of unit $i$ in layer $j$} \newline&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}\end{align}\]</span></p>
<p>If we had one hidden layer, it would look like:</p>
<p><span class="math inline">\(\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline x_3\end{bmatrix}\rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \newline a_3^{(2)} \newline \end{bmatrix}\rightarrow h_\theta(x)\)</span></p>
<p>The values for each of the “activation” nodes is obtained as follows:</p>
<p><span style="color: red">
<span class="math display">\[\begin{align} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align}\]</span>
</span></p>
<p>This is saying that we compute our activation nodes by using a 3×4 matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix <span class="math inline">\(\Theta^{(2)}\)</span> containing the weights for our second layer of nodes.</p>
<p>Each layer gets its own matrix of weights, <span class="math inline">\(\Theta^{(j)}\)</span>.</p>
<p>The dimensions of these matrices of weights is determined as follows:</p>
<p>If network has <span class="math inline">\(s_j\)</span> units in layer <span class="math inline">\(j\)</span> and <span class="math inline">\(s_{j+1}\)</span> units in layer <span class="math inline">\(j+1\)</span>, then <span class="math inline">\(\Theta^{(j)}\)</span> will be of dimension <span class="math inline">\(s_{j+1} \times (s_j + 1)\)</span>.</p>
<p>The +1 comes from the addition in <span class="math inline">\(\Theta^{(j)}\)</span> of the “bias nodes,” <span class="math inline">\(x_0\)</span> and <span class="math inline">\(\Theta_0^{(j)}\)</span>. In other words the output nodes will not include the bias nodes while the inputs will. The following image summarizes our model representation:</p>
<p><img src="Neural_Network.png" /></p>
<p>Example: If layer 1 has 2 input nodes and layer 2 has 4 activation nodes. Dimension of <span class="math inline">\(\Theta^{(1)}\)</span> is going to be 4×3 where <span class="math inline">\(s_j = 2\)</span> and <span class="math inline">\(s_{j+1} = 4\)</span>, so <span class="math inline">\(s_{j+1} \times (s_j + 1) = 4 \times 3\)</span>.</p>
</div>
<div id="model-representation-ii" class="section level2">
<h2><span class="header-section-number">5.2</span> Model Representation II</h2>
<p>To re-iterate, the following is an example of a neural network:</p>
<p><span class="math display">\[\begin{align} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align}\]</span></p>
<p>In this section we’ll do a vectorized implementation of the above functions. We’re going to define a new variable <span class="math inline">\(z_k^{(j)}\)</span> that encompasses the parameters inside our g function. In our previous example if we replaced by the variable z for all the parameters we would get:</p>
<p><span class="math display">\[\begin{align}a_1^{(2)} = g(z_1^{(2)}) \newline a_2^{(2)} = g(z_2^{(2)}) \newline a_3^{(2)} = g(z_3^{(2)}) \newline \end{align}\]</span></p>
<p>In other words, for layer j=2 and node k, the variable z will be:</p>
<p><span class="math inline">\(z_k^{(2)} = \Theta_{k,0}^{(1)}x_0 + \Theta_{k,1}^{(1)}x_1 + \cdots + \Theta_{k,n}^{(1)}x_n\)</span></p>
<p>The vector representation of x and <span class="math inline">\(z^{j}\)</span> is:</p>
<p><span class="math display">\[\begin{align}x = \begin{bmatrix}x_0 \newline x_1 \newline\cdots \newline x_n\end{bmatrix} &amp;z^{(j)} = \begin{bmatrix}z_1^{(j)} \newline z_2^{(j)} \newline\cdots \newline z_n^{(j)}\end{bmatrix}\end{align}\]</span></p>
<p>Setting <span class="math inline">\(x = a^{(1)}\)</span>, we can rewrite the equation as:</p>
<p><span class="math inline">\(z^{(j)} = \Theta^{(j-1)}a^{(j-1)}\)</span></p>
<p>We are multiplying our matrix <span class="math inline">\(\Theta^{(j-1)}\)</span> with dimensions <span class="math inline">\(s_j\times (n+1)\)</span> (where <span class="math inline">\(s_j\)</span> is the number of our activation nodes) by our vector <span class="math inline">\(a^{(j-1)}\)</span> with height (n+1). This gives us our vector <span class="math inline">\(z^{(j)}\)</span> with height <span class="math inline">\(s_j\)</span>. Now we can get a vector of our activation nodes for layer j as follows:</p>
<p><span class="math inline">\(a^{(j)} = g(z^{(j)})\)</span></p>
<p>Where our function g can be applied element-wise to our vector <span class="math inline">\(z^{(j)}\)</span>.</p>
<p>We can then add a bias unit (equal to 1) to layer j after we have computed <span class="math inline">\(a^{(j)}\)</span>. This will be element <span class="math inline">\(a_0^{(j)}\)</span> and will be equal to 1. To compute our final hypothesis, let’s first compute another z vector:</p>
<p><span class="math inline">\(z^{(j+1)} = \Theta^{(j)}a^{(j)}\)</span></p>
<p>We get this final z vector by multiplying the next theta matrix after <span class="math inline">\(\Theta^{(j-1)}\)</span> with the values of all the activation nodes we just got. This last theta matrix <span class="math inline">\(\Theta^{(j)}\)</span> will have only <strong>one row</strong> which is multiplied by one column <span class="math inline">\(a^{(j)}\)</span> so that our result is a single number. We then get our final result with:</p>
<p><span class="math inline">\(h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)})\)</span></p>
<p>Notice that in this <strong>last step</strong>, between layer j and layer j+1, we are doing <strong>exactly the same thing</strong> as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses.</p>
<div id="examples-and-intuitions-i" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Examples and Intuitions I</h3>
<p>A simple example of applying neural networks is by predicting <span class="math inline">\(x_1\)</span> AND <span class="math inline">\(x_2\)</span>, which is the logical ‘and’ operator and is only true if both <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are 1.</p>
<p>The graph of our functions will look like:</p>
<p><span class="math display">\[\begin{align}\begin{bmatrix}x_0 \newline x_1 \newline x_2\end{bmatrix} \rightarrow\begin{bmatrix}g(z^{(2)})\end{bmatrix} \rightarrow h_\Theta(x)\end{align}\]</span></p>
<p>Remember that <span class="math inline">\(x_0\)</span> is our bias variable and is always 1.</p>
<p>Let’s set our first theta matrix as:</p>
<p><span class="math inline">\(\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix}\)</span></p>
<p>This will cause the output of our hypothesis to only be positive if both <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are 1. In other words:</p>
<p><span class="math display">\[\begin{align}&amp; h_\Theta(x) = g(-30 + 20x_1 + 20x_2) \newline \newline &amp; x_1 = 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \newline &amp; x_1 = 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \newline &amp; x_1 = 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \newline &amp; x_1 = 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1\end{align}\]</span></p>
<p>So we have constructed one of the fundamental operations in computers by using a small neural network rather than using an actual AND gate. Neural networks can also be used to simulate all the other logical gates. The following is an example of the logical operator ‘OR’, meaning either <span class="math inline">\(x_1\)</span> is true or <span class="math inline">\(x_2\)</span> is true, or both:</p>
<p><img src="NN_example_OR.png" />
<img src="NN_example_2.png" /></p>
</div>
<div id="examples-and-intuitions-ii" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Examples and Intuitions II</h3>
<p>The <span class="math inline">\(Θ^{(1)}\)</span> matrices for AND, NOR, and OR are:</p>
<p><span class="math display">\[\begin{align}AND:\newline\Theta^{(1)} &amp;=\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix} \newline NOR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}10 &amp; -20 &amp; -20\end{bmatrix} \newline OR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix} \newline\end{align}\]</span></p>
<p>We can combine these to get the XNOR logical operator (which gives 1 if <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are both 0 or both 1).</p>
<p><span class="math display">\[\begin{align}\begin{bmatrix}x_0 \newline x_1 \newline x_2\end{bmatrix} \rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \end{bmatrix} \rightarrow\begin{bmatrix}a^{(3)}\end{bmatrix} \rightarrow h_\Theta(x)\end{align}\]</span></p>
<p>For the transition between the first and second layer, we’ll use a <span class="math inline">\(Θ^{(1)}\)</span> matrix that combines the values for AND and NOR:</p>
<p><span class="math inline">\(\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20 \newline 10 &amp; -20 &amp; -20\end{bmatrix}\)</span></p>
<p>For the transition between the second and third layer, we’ll use a <span class="math inline">\(Θ^{(2)}\)</span> matrix that uses the value for OR:</p>
<p><span class="math inline">\(\Theta^{(2)} =\begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix}\)</span></p>
<p>Let’s write out the values for all our nodes:</p>
<p><span class="math display">\[\begin{align}&amp; a^{(2)} = g(\Theta^{(1)} \cdot x) \newline&amp; a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)}) \newline&amp; h_\Theta(x) = a^{(3)}\end{align}\]</span></p>
<p>And there we have the XNOR operator using a hidden layer with two nodes! The following summarizes the above algorithm:</p>
<p><img src="NN_example_3.png" /></p>
</div>
</div>
<div id="multiclass-classification" class="section level2">
<h2><span class="header-section-number">5.3</span> Multiclass Classification</h2>
<p>To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly:</p>
<p><img src="Multiclass1.png" /></p>
<p>We can define our set of resulting classes as y:</p>
<p><img src="Multiclass2.png" />
Each <span class="math inline">\(y^{(i)}\)</span> represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. The inner layers, each provide us with some new information which leads to our final hypothesis function. The setup looks like:</p>
<p><img src="Multiclass3.png" />
Our resulting hypothesis for one set of inputs may look like:</p>
<p><span class="math inline">\(h_\Theta(x) =\begin{bmatrix}0 \newline 0 \newline 1 \newline 0 \newline\end{bmatrix}\)</span></p>
<p>In which case our resulting class is the third one down, or <span class="math inline">\(h_\Theta(x)_3\)</span>, which represents the motorcycle.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdw.pdf", "bookdw.epub"]
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
